{
  "papers": {
    "2025/01": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/02": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/03": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/04": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/05": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/06": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/07": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/08": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/09": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/10": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/11": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ],
    "2025/12": [
      {
        "paper": {
          "id": "2601.05960",
          "authors": [
            {
              "_id": "6964aa85138cc47cbd7653f1",
              "name": "VÃ­ctor Gallego",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:26:52.000Z",
          "submittedOnDailyAt": "2026-01-12T05:37:03.445Z",
          "title": "Distilling Feedback into Memory-as-a-Tool",
          "submittedOnDailyBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
          "upvotes": 1,
          "discussionId": "6964aa86138cc47cbd7653f2",
          "ai_summary": "A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.",
          "ai_keywords": [
            "LLMs",
            "test-time refinement",
            "inference-time reasoning",
            "file-based memory system",
            "agent-controlled tool calls",
            "Rubric Feedback Bench",
            "rubric-based learning"
          ]
        },
        "publishedAt": "2026-01-09T12:26:52.000Z",
        "title": "Distilling Feedback into Memory-as-a-Tool",
        "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05960.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "5fad8602b8423e1d80b8a965",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
          "fullname": "Victor Gallego",
          "name": "vicgalle",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 140,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04726",
          "authors": [
            {
              "_id": "69649a72138cc47cbd7653d1",
              "name": "Yuyang Hu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d2",
              "name": "Jiongnan Liu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d3",
              "name": "Jiejun Tan",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d4",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "69649a72138cc47cbd7653d5",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:44:07.000Z",
          "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
          "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
          "submittedOnDailyBy": {
            "_id": "6544b9b646dbdeca34ee5f52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
            "isPro": false,
            "fullname": "Yuyang Hu",
            "user": "namespace-ERI",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
          "upvotes": 1,
          "discussionId": "69649a73138cc47cbd7653d6",
          "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
          "ai_keywords": [
            "event-centric memory",
            "Event Segmentation Theory",
            "Event Graph",
            "structured memory",
            "logical relationships",
            "memory organization",
            "long-horizon reasoning",
            "memory retrieval",
            "goal-directed navigation",
            "semantic retrieval"
          ]
        },
        "publishedAt": "2026-01-08T03:44:07.000Z",
        "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
        "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6544b9b646dbdeca34ee5f52",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
          "fullname": "Yuyang Hu",
          "name": "namespace-ERI",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03319",
          "authors": [
            {
              "_id": "6960e7365b7998385e6396e3",
              "user": {
                "_id": "6761f183e5b85d453550147a",
                "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
                "isPro": false,
                "fullname": "Eld Mat",
                "user": "eldad929",
                "type": "user"
              },
              "name": "Eldad Matmon",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e4",
              "name": "Amit Bracha",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e5",
              "name": "Noam Rotstein",
              "hidden": false
            },
            {
              "_id": "6960e7365b7998385e6396e6",
              "name": "Ron Kimmel",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:56:28.000Z",
          "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
          "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
          "submittedOnDailyBy": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
          "upvotes": 17,
          "discussionId": "6960e7365b7998385e6396e7",
          "projectPage": "https://c4ricaturegs.github.io/",
          "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "FLAME mesh",
            "curvature-weighted Poisson equation",
            "pseudo-ground-truth caricature images",
            "local affine transformations",
            "real-time deformations",
            "closed-form solutions"
          ],
          "organization": {
            "_id": "6393322be2364bc1eea56e45",
            "name": "Technion",
            "fullname": "Technion Israel institute of technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
          }
        },
        "publishedAt": "2026-01-06T08:56:28.000Z",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6761f183e5b85d453550147a",
          "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
          "fullname": "Eld Mat",
          "name": "eldad929",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6393322be2364bc1eea56e45",
          "name": "Technion",
          "fullname": "Technion Israel institute of technology",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03017",
          "authors": [
            {
              "_id": "696488cc138cc47cbd765365",
              "name": "Jing Xiong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765366",
              "name": "Qi Han",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765367",
              "name": "Yunta Hsieh",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765368",
              "name": "Hui Shen",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765369",
              "name": "Huajian Xin",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536a",
              "name": "Chaofan Tao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536b",
              "name": "Chenyang Zhao",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536c",
              "name": "Hengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536d",
              "name": "Taiqiang Wu",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536e",
              "name": "Zhen Zhang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd76536f",
              "name": "Haochen Wang",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765370",
              "name": "Zhongwei Wan",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765371",
              "name": "Lingpeng Kong",
              "hidden": false
            },
            {
              "_id": "696488cc138cc47cbd765372",
              "name": "Ngai Wong",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T13:42:51.000Z",
          "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
          "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
          "submittedOnDailyBy": {
            "_id": "60851545a5da133ac6c38686",
            "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
            "isPro": false,
            "fullname": "Jing Xiong",
            "user": "menik1126",
            "type": "user"
          },
          "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
          "upvotes": 64,
          "discussionId": "696488cc138cc47cbd765373",
          "projectPage": "https://mmformalizer.github.io/",
          "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
          "ai_keywords": [
            "autoformalization",
            "multimodal",
            "perceptually grounded primitives",
            "recursive grounding",
            "axiom composition",
            "adaptive recursive termination",
            "dimensional grounding",
            "axiomatic grounding",
            "PhyX-AF",
            "MathVerse",
            "PhyX",
            "Synthetic Geometry",
            "Analytic Geometry",
            "GPT-5",
            "Gemini-3-Pro",
            "classical mechanics",
            "relativity",
            "quantum mechanics",
            "thermodynamics"
          ]
        },
        "publishedAt": "2026-01-06T08:42:51.000Z",
        "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
        "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "60851545a5da133ac6c38686",
          "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
          "fullname": "Jing Xiong",
          "name": "menik1126",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.02760",
          "authors": [
            {
              "_id": "696486dd138cc47cbd765353",
              "name": "Zeyu Ren",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765354",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765355",
              "name": "Wukai Li",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765356",
              "name": "Qingxiang Liu",
              "hidden": false
            },
            {
              "_id": "696486dd138cc47cbd765357",
              "name": "Hao Tang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T06:51:35.000Z",
          "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "submittedOnDailyBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
          "upvotes": 1,
          "discussionId": "696486dd138cc47cbd765358",
          "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
          "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
          "githubRepoAddedBy": "user",
          "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
          "ai_keywords": [
            "DINOv3",
            "visual encoder",
            "dense features",
            "Simple Depth Transformer",
            "SDT",
            "transformer-based decoder",
            "cross-scale feature fusion",
            "parameter efficiency",
            "quality-based filtering strategy",
            "zero-shot monocular depth estimation"
          ],
          "githubStars": 25,
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-06T01:51:35.000Z",
        "title": "AnyDepth: Depth Estimation Made Easy",
        "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64ec877bb93654d4ca5c92e9",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
          "fullname": "Zeyu Zhang",
          "name": "SteveZeyuZhang",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04720",
          "authors": [
            {
              "_id": "69647212138cc47cbd765311",
              "name": "Mingxin Li",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765312",
              "name": "Yanzhao Zhang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765313",
              "name": "Dingkun Long",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765314",
              "name": "Keqin Chen",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765315",
              "name": "Sibo Song",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765316",
              "name": "Shuai Bai",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765317",
              "name": "Zhibo Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765318",
              "name": "Pengjun Xie",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd765319",
              "name": "An Yang",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531a",
              "name": "Dayiheng Liu",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531b",
              "name": "Jingren Zhou",
              "hidden": false
            },
            {
              "_id": "69647212138cc47cbd76531c",
              "name": "Junyang Lin",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T08:36:06.000Z",
          "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
          "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
          "submittedOnDailyBy": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
          "upvotes": 9,
          "discussionId": "69647212138cc47cbd76531d",
          "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
          "githubRepoAddedBy": "auto",
          "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
          "ai_keywords": [
            "Qwen3-VL-Embedding",
            "Qwen3-VL-Reranker",
            "multimodal search",
            "multi-stage training",
            "contrastive pre-training",
            "model distillation",
            "Matryoshka Representation Learning",
            "cross-encoder architecture",
            "cross-attention mechanisms",
            "multimodal embedding",
            "visual question answering",
            "video-text matching"
          ],
          "githubStars": 551,
          "organization": {
            "_id": "64c8b5837fe12ecd0a7e92eb",
            "name": "Qwen",
            "fullname": "Qwen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
          }
        },
        "publishedAt": "2026-01-08T03:36:06.000Z",
        "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
        "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "616adb8578833ce5997e441a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
          "fullname": "Dingkun Long",
          "name": "thenlper",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 121,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64c8b5837fe12ecd0a7e92eb",
          "name": "Qwen",
          "fullname": "Qwen",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04786",
          "authors": [
            {
              "_id": "6964788b138cc47cbd76533c",
              "name": "Lang Feng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533d",
              "name": "Fuchao Yang",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533e",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd76533f",
              "name": "Xin Cheng",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765340",
              "name": "Haiyang Xu",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765341",
              "name": "Zhenglin Wan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765342",
              "name": "Ming Yan",
              "hidden": false
            },
            {
              "_id": "6964788b138cc47cbd765343",
              "name": "Bo An",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:10:20.000Z",
          "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
          "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
          "submittedOnDailyBy": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
            "isPro": false,
            "fullname": "Lang Feng",
            "user": "langfeng01",
            "type": "user"
          },
          "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
          "upvotes": 7,
          "discussionId": "6964788b138cc47cbd765344",
          "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
          "ai_keywords": [
            "large language models",
            "reinforcement learning",
            "multi-turn interaction trajectories",
            "visual tokens",
            "segment optical caching",
            "agentic self-compression",
            "token efficiency",
            "rendering speedup"
          ],
          "organization": {
            "_id": "6508b28cf36bb51c50faad98",
            "name": "NanyangTechnologicalUniversity",
            "fullname": "Nanyang Technological University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
          }
        },
        "publishedAt": "2026-01-08T05:10:20.000Z",
        "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
        "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66ba29dd59e8e7a957154c5f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
          "fullname": "Lang Feng",
          "name": "langfeng01",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 4,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6508b28cf36bb51c50faad98",
          "name": "NanyangTechnologicalUniversity",
          "fullname": "Nanyang Technological University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05930",
          "authors": [
            {
              "_id": "69646f2d138cc47cbd7652db",
              "name": "Jingsheng Zheng",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dc",
              "name": "Jintian Zhang",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652dd",
              "name": "Yujie Luo",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652de",
              "name": "Yuren Mao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652df",
              "name": "Yunjun Gao",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e0",
              "name": "Lun Du",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e1",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646f2d138cc47cbd7652e2",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:44:17.000Z",
          "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
          "title": "Can We Predict Before Executing Machine Learning Agents?",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
          "upvotes": 16,
          "discussionId": "69646f2d138cc47cbd7652e3",
          "githubRepo": "https://github.com/zjunlp/predict-before-execute",
          "githubRepoAddedBy": "user",
          "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
          "ai_keywords": [
            "autonomous machine learning agents",
            "Generate-Execute-Feedback paradigm",
            "Execution Bottleneck",
            "World Models",
            "Data-centric Solution Preference",
            "LLMs",
            "Predict-then-Verify loop",
            "convergence acceleration"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "67c1d682826160b28f778510",
            "name": "antgroup",
            "fullname": "Ant Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
          }
        },
        "publishedAt": "2026-01-09T11:44:17.000Z",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "67c1d682826160b28f778510",
          "name": "antgroup",
          "fullname": "Ant Group",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05905",
          "authors": [
            {
              "_id": "69646dd9138cc47cbd7652c7",
              "name": "Haoming Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c8",
              "name": "Ningyuan Zhao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652c9",
              "name": "Yunzhi Yao",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ca",
              "name": "Weihong Xu",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cb",
              "name": "Hongru Wang",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cc",
              "name": "Xinle Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cd",
              "name": "Shumin Deng",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652ce",
              "name": "Jeff Z. Pan",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652cf",
              "name": "Huajun Chen",
              "hidden": false
            },
            {
              "_id": "69646dd9138cc47cbd7652d0",
              "name": "Ningyu Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T16:23:21.000Z",
          "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
          "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
          "submittedOnDailyBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
          "upvotes": 10,
          "discussionId": "69646dda138cc47cbd7652d1",
          "githubRepo": "https://github.com/zjunlp/belief",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
          "ai_keywords": [
            "Large Language Models",
            "Self-Consistency",
            "belief robustness",
            "conceptual neighborhood",
            "cognitive stress-testing",
            "context-invariant belief structure",
            "long-tail knowledge brittleness",
            "Structure-Aware Training"
          ],
          "githubStars": 2,
          "organization": {
            "_id": "6345aadf5efccdc07f1365a5",
            "name": "ZhejiangUniversity",
            "fullname": "Zhejiang University"
          }
        },
        "publishedAt": "2026-01-09T11:23:21.000Z",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "620b3bbb0668e435407c8d0a",
          "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
          "fullname": "Ningyu Zhang",
          "name": "Ningyu",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 35,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6345aadf5efccdc07f1365a5",
          "name": "ZhejiangUniversity",
          "fullname": "Zhejiang University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05432",
          "authors": [
            {
              "_id": "69646268138cc47cbd76527e",
              "name": "Yuxiang Ji",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd76527f",
              "name": "Yong Wang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765280",
              "name": "Ziyu Ma",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765281",
              "name": "Yiming Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765282",
              "name": "Hailang Huang",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765283",
              "name": "Xuecai Hu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765284",
              "name": "Guanhua Chen",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765285",
              "name": "Liaoni Wu",
              "hidden": false
            },
            {
              "_id": "69646268138cc47cbd765286",
              "name": "Xiangxiang Chu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T23:47:30.000Z",
          "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
          "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
          "submittedOnDailyBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
          "upvotes": 64,
          "discussionId": "69646268138cc47cbd765287",
          "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
          "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
          "githubRepoAddedBy": "user",
          "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
          "ai_keywords": [
            "vision-language model",
            "geolocalization",
            "chain-of-thought reasoning",
            "agentic capabilities",
            "agentic reinforcement learning",
            "parallel test-time scaling",
            "agent-in-the-map loop",
            "MAPBench",
            "Acc@500m"
          ],
          "githubStars": 35,
          "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
          }
        },
        "publishedAt": "2026-01-08T18:47:30.000Z",
        "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
        "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "66d255e3947594430c723ff6",
          "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
          "fullname": "xiaochonglinghu",
          "name": "xiaochonglinghu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64488b334988ee01f2a8d856",
          "name": "alibaba-inc",
          "fullname": "alibaba-inc",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06002",
          "authors": [
            {
              "_id": "6964644c138cc47cbd76529b",
              "name": "Qiguang Chen",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529c",
              "name": "Yantao Du",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529d",
              "name": "Ziniu Li",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529e",
              "name": "Jinhao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd76529f",
              "name": "Songyao Duan",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a0",
              "name": "Jiarui Guo",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a1",
              "name": "Minghao Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a2",
              "name": "Jiaheng Liu",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a3",
              "name": "Tong Yang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a4",
              "name": "Ge Zhang",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a5",
              "name": "Libo Qin",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a6",
              "name": "Wanxiang Che",
              "hidden": false
            },
            {
              "_id": "6964644c138cc47cbd7652a7",
              "name": "Wenhao Huang",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
          ],
          "publishedAt": "2026-01-09T18:39:01.000Z",
          "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
          "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
          "submittedOnDailyBy": {
            "_id": "636f526a6cd69d9a36ff2b53",
            "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
            "isPro": false,
            "fullname": "Qiguang Chen",
            "user": "LightChen2333",
            "type": "user"
          },
          "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
          "upvotes": 30,
          "discussionId": "6964644c138cc47cbd7652a8",
          "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
          "ai_keywords": [
            "chain-of-thought",
            "large language models",
            "Long CoT",
            "fine-tuning",
            "entropy convergence",
            "semantic isomers",
            "distribution-transfer-graph",
            "molecular-like structures",
            "deep reasoning",
            "self-reflection",
            "self-exploration"
          ],
          "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
          }
        },
        "publishedAt": "2026-01-09T13:39:01.000Z",
        "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
        "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "636f526a6cd69d9a36ff2b53",
          "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
          "fullname": "Qiguang Chen",
          "name": "LightChen2333",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "653b817d32c97d0655575872",
          "name": "ByteDance",
          "fullname": "ByteDance",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05851",
          "authors": [
            {
              "_id": "69646ab0138cc47cbd7652ad",
              "name": "Sandeep Mishra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652ae",
              "name": "Devichand Budagam",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652af",
              "name": "Anubhab Mandal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b0",
              "name": "Bishal Santra",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b1",
              "name": "Pawan Goyal",
              "hidden": false
            },
            {
              "_id": "69646ab0138cc47cbd7652b2",
              "name": "Manish Gupta",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:29:50.000Z",
          "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
          "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
          "submittedOnDailyBy": {
            "_id": "653f2d18eefdcdc9fdb446b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
            "isPro": false,
            "fullname": "Devichand Budagam",
            "user": "devichand",
            "type": "user"
          },
          "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
          "upvotes": 0,
          "discussionId": "69646ab0138cc47cbd7652b3",
          "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
          "ai_keywords": [
            "multimodal auto-completion",
            "vision-language models",
            "VLMs",
            "router framework",
            "multimodal context",
            "real-time prediction",
            "user satisfaction",
            "user typing effort",
            "multi-turn conversations"
          ],
          "organization": {
            "_id": "61bbf43e544c3241f255e0fe",
            "name": "IITKGP",
            "fullname": "Indian Institute of Technology, Kharagpur"
          }
        },
        "publishedAt": "2026-01-09T10:29:50.000Z",
        "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
        "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "653f2d18eefdcdc9fdb446b3",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
          "fullname": "Devichand Budagam",
          "name": "devichand",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bbf43e544c3241f255e0fe",
          "name": "IITKGP",
          "fullname": "Indian Institute of Technology, Kharagpur"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04888",
          "authors": [
            {
              "_id": "696079e75b7998385e639515",
              "user": {
                "_id": "6883352ff256918bd89942b5",
                "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
                "isPro": false,
                "fullname": "TongyuWen",
                "user": "vvv111222",
                "type": "user"
              },
              "name": "Tongyu Wen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639516",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696079e75b7998385e639517",
              "name": "Zhicheng Dou",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:39:05.000Z",
          "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
          "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
          "submittedOnDailyBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "isPro": true,
            "fullname": "Rajkumar rawal",
            "user": "rajkumarrawal",
            "type": "user"
          },
          "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
          "upvotes": 5,
          "discussionId": "696079e75b7998385e639518",
          "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
          "githubRepoAddedBy": "user",
          "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
          "ai_keywords": [
            "search agents",
            "large language models",
            "process rewards",
            "dual-level credit assessment",
            "query refinement",
            "curriculum learning",
            "intermediate search queries",
            "retrieval results"
          ],
          "githubStars": 8
        },
        "publishedAt": "2026-01-08T07:39:05.000Z",
        "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
        "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "64b8e82aa62c52b252c827fa",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
          "fullname": "Rajkumar rawal",
          "name": "rajkumarrawal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 46,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05573",
          "authors": [
            {
              "_id": "69646388138cc47cbd765291",
              "name": "Zehan Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765292",
              "name": "Ziang Zhang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765293",
              "name": "Jiayang Xu",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765294",
              "name": "Jialei Wang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765295",
              "name": "Tianyu Pang",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765296",
              "name": "Chao Du",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765297",
              "name": "HengShuang Zhao",
              "hidden": false
            },
            {
              "_id": "69646388138cc47cbd765298",
              "name": "Zhou Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T06:43:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
          "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
          "submittedOnDailyBy": {
            "_id": "65b36a383a41095a56d0736d",
            "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
            "isPro": false,
            "fullname": "ZiangZhang",
            "user": "Viglong",
            "type": "user"
          },
          "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
          "upvotes": 6,
          "discussionId": "69646388138cc47cbd765299",
          "projectPage": "https://orient-anythingv2.github.io/",
          "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
          "githubRepoAddedBy": "user",
          "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
          "ai_keywords": [
            "generative models",
            "3D assets",
            "model-in-the-loop annotation",
            "symmetry-aware",
            "periodic distribution fitting",
            "multi-frame architecture",
            "relative rotation prediction",
            "zero-shot performance",
            "6DoF pose estimation",
            "object symmetry recognition"
          ],
          "githubStars": 25
        },
        "publishedAt": "2026-01-09T01:43:59.000Z",
        "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
        "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "65b36a383a41095a56d0736d",
          "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
          "fullname": "ZiangZhang",
          "name": "Viglong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05808",
          "authors": [
            {
              "_id": "696462fd138cc47cbd765289",
              "name": "Xiaoshuai Song",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528a",
              "name": "Haofei Chang",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528b",
              "name": "Guanting Dong",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528c",
              "name": "Yutao Zhu",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528d",
              "name": "Zhicheng Dou",
              "hidden": false
            },
            {
              "_id": "696462fd138cc47cbd76528e",
              "name": "Ji-Rong Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T14:32:06.000Z",
          "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
          "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
          "submittedOnDailyBy": {
            "_id": "6621ec2524eb2673fe0790fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
            "isPro": false,
            "fullname": "Ania Forge",
            "user": "zhangboguodong",
            "type": "user"
          },
          "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
          "upvotes": 22,
          "discussionId": "696462fe138cc47cbd76528f",
          "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
          "ai_keywords": [
            "tool-interaction environments",
            "programmatic synthesis",
            "environment skeletons",
            "task scenarios",
            "rule-based trajectory validation",
            "supervised fine-tuning",
            "reinforcement learning",
            "multi-turn interactions",
            "multi-tool interactions"
          ]
        },
        "publishedAt": "2026-01-09T09:32:06.000Z",
        "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
        "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
        "numComments": 4,
        "submittedBy": {
          "_id": "6621ec2524eb2673fe0790fc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
          "fullname": "Ania Forge",
          "name": "zhangboguodong",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 6,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04823",
          "authors": [
            {
              "_id": "6964607c138cc47cbd765259",
              "name": "Guanzhi Deng",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525a",
              "name": "Bo Li",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525b",
              "name": "Ronghao Chen",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525c",
              "name": "Huacan Wang",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525d",
              "name": "Linqi Song",
              "hidden": false
            },
            {
              "_id": "6964607c138cc47cbd76525e",
              "name": "Lijie Wen",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:58:51.000Z",
          "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
          "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
          "submittedOnDailyBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
          "upvotes": 2,
          "discussionId": "6964607c138cc47cbd76525f",
          "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
          "ai_keywords": [
            "Mixture-of-Experts",
            "Large Language Models",
            "parameter-efficient fine-tuning",
            "LoRA",
            "dynamic rank allocation",
            "expert routing frequency",
            "expert saliency scoring",
            "heterogeneous rank distribution"
          ]
        },
        "publishedAt": "2026-01-08T05:58:51.000Z",
        "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
        "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6582c482f3006507ea10302a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
          "fullname": "Bo Li",
          "name": "liboaccn",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05870",
          "authors": [
            {
              "_id": "69646166138cc47cbd765271",
              "name": "Huilin Deng",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765272",
              "name": "Hongchen Luo",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765273",
              "name": "Yue Zhu",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765274",
              "name": "Long Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765275",
              "name": "Zhuoyue Chen",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765276",
              "name": "Xinghao Zhao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765277",
              "name": "Ming Li",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765278",
              "name": "Jihai Zhang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd765279",
              "name": "Mengchang Wang",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527a",
              "name": "Yang Cao",
              "hidden": false
            },
            {
              "_id": "69646166138cc47cbd76527b",
              "name": "Yu Kang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T15:46:40.000Z",
          "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
          "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
          "upvotes": 0,
          "discussionId": "69646166138cc47cbd76527c",
          "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
          "ai_keywords": [
            "Reinforcement Learning with Verifiable Rewards",
            "large language models",
            "exploration collapse",
            "policy entropy",
            "reward hacking",
            "latent policy optimization",
            "iterative information bottleneck",
            "topological branching",
            "reasoning trajectories",
            "information bottleneck principle",
            "mathematical reasoning benchmarks"
          ]
        },
        "publishedAt": "2026-01-09T10:46:40.000Z",
        "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
        "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05503",
          "authors": [
            {
              "_id": "69646131138cc47cbd765268",
              "name": "Roy Xie",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd765269",
              "name": "Deepak Gopinath",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526a",
              "name": "David Qiu",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526b",
              "name": "Dong Lin",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526c",
              "name": "Haitian Sun",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526d",
              "name": "Saloni Potdar",
              "hidden": false
            },
            {
              "_id": "69646131138cc47cbd76526e",
              "name": "Bhuwan Dhingra",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T03:24:46.000Z",
          "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
          "title": "Over-Searching in Search-Augmented Large Language Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
          "upvotes": 1,
          "discussionId": "69646131138cc47cbd76526f",
          "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
          "githubRepoAddedBy": "user",
          "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
          "ai_keywords": [
            "search-augmented large language models",
            "over-searching",
            "retrieval",
            "hallucinations",
            "Tokens Per Correctness",
            "multi-turn conversations",
            "answerable queries",
            "unanswerable queries",
            "complex reasoning models",
            "deep research systems",
            "noisy retrieval",
            "negative evidence"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-08T22:24:46.000Z",
        "title": "Over-Searching in Search-Augmented Large Language Models",
        "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.06021",
          "authors": [
            {
              "_id": "69645f30138cc47cbd765248",
              "name": "Jiajie Zhang",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd765249",
              "name": "Xin Lv",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524a",
              "name": "Ling Feng",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524b",
              "name": "Lei Hou",
              "hidden": false
            },
            {
              "_id": "69645f30138cc47cbd76524c",
              "name": "Juanzi Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T18:57:53.000Z",
          "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
          "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
          "submittedOnDailyBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
          "upvotes": 26,
          "discussionId": "69645f30138cc47cbd76524d",
          "githubRepo": "https://github.com/THUDM/CaRR",
          "githubRepoAddedBy": "user",
          "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
          "ai_keywords": [
            "reinforcement learning",
            "deep search agents",
            "fine-grained reward framework",
            "reasoning comprehensiveness",
            "factual grounding",
            "evidence connectivity",
            "verifiable single-hop rubrics",
            "citation-aware group relative policy optimization",
            "outcome rewards",
            "shortcut exploitation",
            "hallucinations"
          ],
          "githubStars": 3,
          "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
          }
        },
        "publishedAt": "2026-01-09T13:57:53.000Z",
        "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
        "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66cdd285c51a915bd5f2d017",
          "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
          "fullname": "Jiajie Zhang",
          "name": "NeoZ123",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "62ad27f19096e7f9ecb1853a",
          "name": "zai-org",
          "fullname": "Z.ai",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05966",
          "authors": [
            {
              "_id": "69645e19138cc47cbd76523f",
              "name": "Longbin Ji",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765240",
              "name": "Xiaoxiong Liu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765241",
              "name": "Junyuan Shang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765242",
              "name": "Shuohuan Wang",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765243",
              "name": "Yu Sun",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765244",
              "name": "Hua Wu",
              "hidden": false
            },
            {
              "_id": "69645e19138cc47cbd765245",
              "name": "Haifeng Wang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T17:34:59.000Z",
          "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
          "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
          "upvotes": 11,
          "discussionId": "69645e19138cc47cbd765246",
          "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
          "ai_keywords": [
            "Visual Autoregressive",
            "diffusion models",
            "flow-matching models",
            "video generation",
            "multi-scale next-frame prediction",
            "autoregressive modeling",
            "intra-frame VAR modeling",
            "causal next-frame prediction",
            "3D multi-scale tokenizer",
            "temporal dependencies",
            "spatial dependencies",
            "Multi-scale Temporal RoPE",
            "Cross-Frame Error Correction",
            "Random Frame Mask",
            "multi-stage pretraining pipeline",
            "FVD",
            "VBench"
          ]
        },
        "publishedAt": "2026-01-09T12:34:59.000Z",
        "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
        "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05848",
          "authors": [
            {
              "_id": "69645dac138cc47cbd76522e",
              "name": "Nate Gillman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd76522f",
              "name": "Yinghua Zhou",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765230",
              "name": "Zitian Tang",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765231",
              "name": "Evan Luo",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765232",
              "name": "Arjan Chakravarthy",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765233",
              "name": "Daksh Aggarwal",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765234",
              "name": "Michael Freeman",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765235",
              "name": "Charles Herrmann",
              "hidden": false
            },
            {
              "_id": "69645dac138cc47cbd765236",
              "name": "Chen Sun",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
          ],
          "publishedAt": "2026-01-09T15:23:36.000Z",
          "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
          "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
          "upvotes": 4,
          "discussionId": "69645dad138cc47cbd765237",
          "projectPage": "https://goal-force.github.io/",
          "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
          "ai_keywords": [
            "video generation",
            "world models",
            "causal primitives",
            "force vectors",
            "physics simulation",
            "zero-shot generalization",
            "neural physics simulators"
          ]
        },
        "publishedAt": "2026-01-09T10:23:36.000Z",
        "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
        "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05637",
          "authors": [
            {
              "_id": "69645d4e138cc47cbd765225",
              "name": "Emily Cheng",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765226",
              "name": "Carmen Amo Alonso",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765227",
              "name": "Federico Danieli",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765228",
              "name": "Arno Blaas",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd765229",
              "name": "Luca Zappella",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522a",
              "name": "Pau Rodriguez",
              "hidden": false
            },
            {
              "_id": "69645d4e138cc47cbd76522b",
              "name": "Xavier Suau",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-09T08:50:02.000Z",
          "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
          "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
          "upvotes": 2,
          "discussionId": "69645d4e138cc47cbd76522c",
          "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
          "ai_keywords": [
            "generative models",
            "controllable sets",
            "control process",
            "dialogue setting",
            "probably-approximately correct bounds",
            "black-box nonlinear control system",
            "language models",
            "text-to-image generation"
          ],
          "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
          }
        },
        "publishedAt": "2026-01-09T03:50:02.000Z",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
        "numComments": 0,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "628cbd99ef14f971b69948ab",
          "name": "apple",
          "fullname": "Apple",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04342",
          "authors": [
            {
              "_id": "69612062138cc47cbd764dba",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "69612062138cc47cbd764dbb",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:40.829Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T19:26:30.000Z",
          "submittedOnDailyAt": "2026-01-09T17:29:59.072Z",
          "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
          "upvotes": 3,
          "discussionId": "69612062138cc47cbd764dbc",
          "projectPage": "https://qualcomm-ai-research.github.io/rehyat",
          "ai_summary": "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.",
          "ai_keywords": [
            "video diffusion models",
            "transformer-based architectures",
            "softmax attention",
            "linear attention",
            "recurrent hybrid attention",
            "chunk-wise recurrent reformulation",
            "constant memory usage",
            "distillation",
            "attention complexity",
            "quadratic attention",
            "long-duration video generation",
            "on-device video generation"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-07T14:26:30.000Z",
        "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
        "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04342.png",
        "numComments": 5,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04792",
          "authors": [
            {
              "_id": "6960f4db138cc47cbd764d6d",
              "name": "Denis Korzhenkov",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6e",
              "name": "Adil Karjauv",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d6f",
              "name": "Animesh Karnewar",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d70",
              "name": "Mohsen Ghafoorian",
              "hidden": false
            },
            {
              "_id": "6960f4db138cc47cbd764d71",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:49.633Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T10:16:06.000Z",
          "submittedOnDailyAt": "2026-01-09T16:57:59.819Z",
          "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
          "upvotes": 2,
          "discussionId": "6960f4db138cc47cbd764d72",
          "projectPage": "https://qualcomm-ai-research.github.io/PyramidalWan",
          "ai_summary": "Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.",
          "ai_keywords": [
            "pyramidal models",
            "diffusion models",
            "forward diffusion process",
            "backward diffusion process",
            "multi-step denoising models",
            "inference efficiency",
            "step distillation",
            "pretrained diffusion model",
            "low-cost fine-tuning"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T05:16:06.000Z",
        "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
        "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04792.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02702",
          "authors": [
            {
              "_id": "69615195138cc47cbd764e4e",
              "name": "Shuhaib Mehri",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e4f",
              "name": "Priyanka Kargupta",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e50",
              "name": "Tal August",
              "hidden": false
            },
            {
              "_id": "69615195138cc47cbd764e51",
              "name": "Dilek Hakkani-TÃ¼r",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T04:26:22.000Z",
          "submittedOnDailyAt": "2026-01-09T16:37:35.975Z",
          "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
          "submittedOnDailyBy": {
            "_id": "64bf5facd05a97d722e68dbc",
            "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
            "isPro": false,
            "fullname": "Shuhaib Mehri",
            "user": "shuhaibmehri",
            "type": "user"
          },
          "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
          "upvotes": 1,
          "discussionId": "69615196138cc47cbd764e52",
          "ai_summary": "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.",
          "ai_keywords": [
            "multi-session collaboration",
            "user preference learning",
            "memory systems",
            "agent adaptation",
            "task success rate",
            "user experience"
          ]
        },
        "publishedAt": "2026-01-05T23:26:22.000Z",
        "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
        "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02702.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bf5facd05a97d722e68dbc",
          "avatarUrl": "/avatars/3289370901a1c0c866f29b97416528d6.svg",
          "fullname": "Shuhaib Mehri",
          "name": "shuhaibmehri",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05149",
          "authors": [
            {
              "_id": "69611c7f138cc47cbd764db5",
              "name": "Elia Peruzzo",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db6",
              "name": "Guillaume SautiÃ¨re",
              "hidden": false
            },
            {
              "_id": "69611c7f138cc47cbd764db7",
              "user": {
                "_id": "628372aa09aa80237c6d2044",
                "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
                "isPro": false,
                "fullname": "Amir Habibian",
                "user": "habibian",
                "type": "user"
              },
              "name": "Amirhossein Habibian",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:42.627Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:39:35.000Z",
          "submittedOnDailyAt": "2026-01-09T12:58:00.294Z",
          "title": "Multi-Scale Local Speculative Decoding for Image Generation",
          "submittedOnDailyBy": {
            "_id": "628372aa09aa80237c6d2044",
            "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
            "isPro": false,
            "fullname": "Amir Habibian",
            "user": "habibian",
            "type": "user"
          },
          "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
          "upvotes": 2,
          "discussionId": "69611c80138cc47cbd764db8",
          "projectPage": "https://qualcomm-ai-research.github.io/mulo-sd-webpage/",
          "ai_summary": "Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.",
          "ai_keywords": [
            "autoregressive models",
            "speculative decoding",
            "multi-resolution drafting",
            "spatially informed verification",
            "up-samplers",
            "candidate image tokens",
            "parallel verification",
            "local rejection",
            "resampling",
            "spatial neighborhoods",
            "GenEval",
            "DPG-Bench",
            "FID",
            "HPSv2",
            "MS-COCO"
          ],
          "organization": {
            "_id": "616851aa840fa49535b3d5b2",
            "name": "qualcomm",
            "fullname": "Qualcomm",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
          }
        },
        "publishedAt": "2026-01-08T12:39:35.000Z",
        "title": "Multi-Scale Local Speculative Decoding for Image Generation",
        "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05149.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "628372aa09aa80237c6d2044",
          "avatarUrl": "/avatars/c1e34dee217d73f5539a0807adc12c80.svg",
          "fullname": "Amir Habibian",
          "name": "habibian",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "616851aa840fa49535b3d5b2",
          "name": "qualcomm",
          "fullname": "Qualcomm",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c2710dc79c1a6e4d22734d/kB7OTGVsC1DPMIV2femsf.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04575",
          "authors": [
            {
              "_id": "696062ce5b7998385e639442",
              "user": {
                "_id": "663a4918e2a8a70966782fac",
                "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
                "isPro": false,
                "fullname": "yuguang",
                "user": "guaguaa",
                "type": "user"
              },
              "name": "Yuguang Yue",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:30.380Z",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639443",
              "name": "Irakli Salia",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639444",
              "name": "Samuel Hunt",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639445",
              "name": "Chris Green",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639446",
              "name": "Wenzhe Shi",
              "hidden": false
            },
            {
              "_id": "696062ce5b7998385e639447",
              "name": "Jonathan J Hunt",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T04:06:17.000Z",
          "submittedOnDailyAt": "2026-01-09T12:38:38.513Z",
          "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
          "submittedOnDailyBy": {
            "_id": "663a4918e2a8a70966782fac",
            "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
            "isPro": false,
            "fullname": "yuguang",
            "user": "guaguaa",
            "type": "user"
          },
          "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
          "upvotes": 2,
          "discussionId": "696062cf5b7998385e639448",
          "projectPage": "https://elefant-ai.github.io/open-p2p/",
          "githubRepo": "https://github.com/elefant-ai/open-p2p",
          "githubRepoAddedBy": "user",
          "ai_summary": "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.",
          "ai_keywords": [
            "behavior cloning",
            "foundation model",
            "realtime inference",
            "human gameplay",
            "scaling laws",
            "causal reasoning",
            "model parameters",
            "training data",
            "video game playing"
          ],
          "githubStars": 39,
          "organization": {
            "_id": "6647bf2ef604081903e6d176",
            "name": "elefantai",
            "fullname": "Elefant AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
          }
        },
        "publishedAt": "2026-01-07T23:06:17.000Z",
        "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
        "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04575.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663a4918e2a8a70966782fac",
          "avatarUrl": "/avatars/bd3ba99417688cf0c36920c3cb8b7309.svg",
          "fullname": "yuguang",
          "name": "guaguaa",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6647bf2ef604081903e6d176",
          "name": "elefantai",
          "fullname": "Elefant AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663a4918e2a8a70966782fac/oP-8fkbIH8kdHAZYJS_9w.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04233",
          "authors": [
            {
              "_id": "6960750c5b7998385e6394e6",
              "name": "Zhiyuan Zhao",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e7",
              "name": "Lijian Lin",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e8",
              "name": "Ye Zhu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394e9",
              "name": "Kai Xie",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394ea",
              "name": "Yunfei Liu",
              "hidden": false
            },
            {
              "_id": "6960750c5b7998385e6394eb",
              "name": "Yu Li",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-04T03:43:52.000Z",
          "submittedOnDailyAt": "2026-01-09T11:40:21.519Z",
          "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
          "submittedOnDailyBy": {
            "_id": "647ede3fd26579210f58b5a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
            "isPro": true,
            "fullname": "ZZY",
            "user": "Approximetal",
            "type": "user"
          },
          "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
          "upvotes": 1,
          "discussionId": "6960750c5b7998385e6394ec",
          "ai_summary": "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.",
          "ai_keywords": [
            "multilingual speech corpus",
            "word-level timestamps",
            "data processing pipeline",
            "non-autoregressive flow-matching framework",
            "zero-shot multilingual synthesis",
            "accent-adversarial training",
            "CTC loss",
            "autoregressive decoder-only architecture",
            "masked token infilling",
            "speech editing",
            "adaptive decoding strategies"
          ],
          "organization": {
            "_id": "692d907ec68dd3f60d37c8b4",
            "name": "LEMAS-Project",
            "fullname": "LEMAS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
          }
        },
        "publishedAt": "2026-01-03T22:43:52.000Z",
        "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
        "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04233.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "647ede3fd26579210f58b5a0",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647ede3fd26579210f58b5a0/MF6fm6WjLCOipAfUomdCU.png",
          "fullname": "ZZY",
          "name": "Approximetal",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "692d907ec68dd3f60d37c8b4",
          "name": "LEMAS-Project",
          "fullname": "LEMAS",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647ede3fd26579210f58b5a0/TJGQyxXQ_NEs0SAdjNc3m.jpeg"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.03111",
          "authors": [
            {
              "_id": "6960f974138cc47cbd764d87",
              "name": "Yiyuan Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d88",
              "name": "Zhen Huang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d89",
              "name": "Yanan Wu",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8a",
              "name": "Weixun Wang",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8b",
              "name": "Xuefeng Li",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8c",
              "name": "Yijia Luo",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8d",
              "name": "Wenbo Su",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8e",
              "name": "Bo Zheng",
              "hidden": false
            },
            {
              "_id": "6960f974138cc47cbd764d8f",
              "name": "Pengfei Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T15:41:35.000Z",
          "submittedOnDailyAt": "2026-01-09T10:50:07.218Z",
          "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
          "submittedOnDailyBy": {
            "_id": "63ae2a83a31e9ea8298d7708",
            "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
            "isPro": false,
            "fullname": "Yiyuan Li",
            "user": "billli",
            "type": "user"
          },
          "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
          "upvotes": 8,
          "discussionId": "6960f975138cc47cbd764d90",
          "ai_summary": "Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.",
          "ai_keywords": [
            "reinforcement learning",
            "large language models",
            "one-shot learning",
            "polymath learning",
            "mathematical reasoning",
            "sample engineering"
          ]
        },
        "publishedAt": "2026-01-06T10:41:35.000Z",
        "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
        "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03111.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ae2a83a31e9ea8298d7708",
          "avatarUrl": "/avatars/d7a9337f05f6276f3773f4bdffb49e42.svg",
          "fullname": "Yiyuan Li",
          "name": "billli",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 5,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05172",
          "authors": [
            {
              "_id": "6960fb36138cc47cbd764d92",
              "name": "Haoyu Zhao",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d93",
              "name": "Akide Liu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d94",
              "name": "Zeyu Zhang",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d95",
              "user": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
              },
              "name": "Weijie Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:47.523Z",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d96",
              "name": "Feng Chen",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d97",
              "name": "Ruihan Zhu",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d98",
              "name": "Gholamreza Haffari",
              "hidden": false
            },
            {
              "_id": "6960fb36138cc47cbd764d99",
              "name": "Bohan Zhuang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:59:42.000Z",
          "submittedOnDailyAt": "2026-01-09T10:28:12.756Z",
          "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
          "submittedOnDailyBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
          "upvotes": 8,
          "discussionId": "6960fb37138cc47cbd764d9a",
          "githubRepo": "https://github.com/ziplab/CoV",
          "githubRepoAddedBy": "user",
          "ai_summary": "Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.",
          "ai_keywords": [
            "embodied question answering",
            "vision-language models",
            "view selection agent",
            "coarse-to-fine exploration",
            "test-time reasoning",
            "3D environment",
            "spatial reasoning",
            "camera actions",
            "open-view search",
            "model-agnostic"
          ],
          "githubStars": 9,
          "organization": {
            "_id": "65a7c89210342794b7a6fe33",
            "name": "ziplab",
            "fullname": "ZIP Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
          }
        },
        "publishedAt": "2026-01-08T12:59:42.000Z",
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05172.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "66699aa8a33847217b5a49c7",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
          "fullname": "Weijie Wang",
          "name": "lhmd",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "65a7c89210342794b7a6fe33",
          "name": "ziplab",
          "fullname": "ZIP Lab",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/629d55b53a3221bb2117422c/ej2edFF8BSXdkVTlVJmeC.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.21815",
          "authors": [
            {
              "_id": "6960e5825b7998385e6396da",
              "name": "Mengqi He",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396db",
              "name": "Xinyu Tian",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dc",
              "name": "Xin Shen",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396dd",
              "user": {
                "_id": "64c71a5647418a0a59e5c7cb",
                "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                "isPro": false,
                "fullname": "Jinhong Ni",
                "user": "mcleanie",
                "type": "user"
              },
              "name": "Jinhong Ni",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:14.555Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396de",
              "name": "Shu Zou",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396df",
              "user": {
                "_id": "6364187d969bdae89e12b209",
                "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg",
                "isPro": false,
                "fullname": "zhaoyuan yang",
                "user": "zhaoyuan",
                "type": "user"
              },
              "name": "Zhaoyuan Yang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:57:08.195Z",
              "hidden": false
            },
            {
              "_id": "6960e5825b7998385e6396e0",
              "name": "Jing Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-26T01:01:25.000Z",
          "submittedOnDailyAt": "2026-01-09T09:01:04.074Z",
          "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
          "submittedOnDailyBy": {
            "_id": "68a3f14e6dd0e4c74c014853",
            "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
            "isPro": true,
            "fullname": "Hubert",
            "user": "ANUHW",
            "type": "user"
          },
          "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
          "upvotes": 20,
          "discussionId": "6960e5835b7998385e6396e1",
          "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.",
          "ai_keywords": [
            "vision-language models",
            "entropy",
            "adversarial attacks",
            "autoregressive generation",
            "high-entropy tokens",
            "semantic degradation",
            "attack success rates",
            "transferability"
          ],
          "organization": {
            "_id": "64ed4ba2453a4b4bef2664c5",
            "name": "anu-cvml",
            "fullname": "Australian National University"
          }
        },
        "publishedAt": "2025-12-25T20:01:25.000Z",
        "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
        "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "68a3f14e6dd0e4c74c014853",
          "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg",
          "fullname": "Hubert",
          "name": "ANUHW",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "64ed4ba2453a4b4bef2664c5",
          "name": "anu-cvml",
          "fullname": "Australian National University"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05125",
          "authors": [
            {
              "_id": "6960d5bd5b7998385e6396cc",
              "user": {
                "_id": "63ee535a190ddd6214f30dc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
                "isPro": false,
                "fullname": "de Rodrigo",
                "user": "de-Rodrigo",
                "type": "user"
              },
              "name": "Ignacio de Rodrigo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:59.881Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396cd",
              "user": {
                "_id": "645c966858f9ee315142a2e5",
                "avatarUrl": "/avatars/51928db15dddc07a51cc35a402f511c7.svg",
                "isPro": false,
                "fullname": "Alvaro Lopez",
                "user": "alvlopez",
                "type": "user"
              },
              "name": "Alvaro J. Lopez-Lopez",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:45:57.718Z",
              "hidden": false
            },
            {
              "_id": "6960d5bd5b7998385e6396ce",
              "name": "Jaime Boal",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
          ],
          "publishedAt": "2026-01-08T17:15:15.000Z",
          "submittedOnDailyAt": "2026-01-09T08:00:42.145Z",
          "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
          "submittedOnDailyBy": {
            "_id": "63ee535a190ddd6214f30dc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
            "isPro": false,
            "fullname": "de Rodrigo",
            "user": "de-Rodrigo",
            "type": "user"
          },
          "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
          "upvotes": 0,
          "discussionId": "6960d5be5b7998385e6396cf",
          "projectPage": "https://huggingface.co/spaces/de-Rodrigo/Embeddings",
          "githubRepo": "https://github.com/nachoDRT/VrDU-Doctor",
          "githubRepoAddedBy": "user",
          "ai_summary": "VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.",
          "ai_keywords": [
            "Vision-Language Models",
            "visual embedding space",
            "latent representations",
            "synthetic data generation",
            "MERIT Dataset",
            "F1 performance",
            "Donut",
            "Idefics2",
            "GPT-4",
            "Pixtral"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "649aaee1c9e0f93d73de930a",
            "name": "CICLAB-Comillas",
            "fullname": "CICLAB Comillas ICAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
          }
        },
        "publishedAt": "2026-01-08T12:15:15.000Z",
        "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
        "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/iEwNzpl0hchE1qOB66urp.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/eBXTQpLOetwpWn6AKVXak.png",
          "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/0qzspboTP0KyIQiaaWq2F.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05125.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "63ee535a190ddd6214f30dc2",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ee535a190ddd6214f30dc2/f4pNRFs9XVLToponWndWJ.jpeg",
          "fullname": "de Rodrigo",
          "name": "de-Rodrigo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "649aaee1c9e0f93d73de930a",
          "name": "CICLAB-Comillas",
          "fullname": "CICLAB Comillas ICAI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee535a190ddd6214f30dc2/q7Rmi9aIAyxGdYCpv08Mx.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04754",
          "authors": [
            {
              "_id": "6960a3f15b7998385e639604",
              "user": {
                "_id": "695f7dcd6b694e775adbdffb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
                "isPro": false,
                "fullname": "Remi Chiou",
                "user": "remiii25",
                "type": "user"
              },
              "name": "Yen-Jen Chiou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:09.338Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639605",
              "user": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
              },
              "name": "Wei-Tse Cheng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:02.092Z",
              "hidden": false
            },
            {
              "_id": "6960a3f15b7998385e639606",
              "name": "Yuan-Fu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:20:46.000Z",
          "submittedOnDailyAt": "2026-01-09T07:46:39.769Z",
          "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
          "submittedOnDailyBy": {
            "_id": "695f7dcd6b694e775adbdffb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
            "isPro": false,
            "fullname": "Remi Chiou",
            "user": "remiii25",
            "type": "user"
          },
          "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
          "upvotes": 2,
          "discussionId": "6960a3f15b7998385e639607",
          "projectPage": "https://chiou1203.github.io/ProFuse/",
          "githubRepo": "https://github.com/chiou1203/ProFuse",
          "githubRepoAddedBy": "user",
          "ai_summary": "ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.",
          "ai_keywords": [
            "3D Gaussian Splatting",
            "open-vocabulary 3D scene understanding",
            "cross-view consistency",
            "intra-mask cohesion",
            "direct registration",
            "dense correspondence-guided pre-registration",
            "3D Context Proposals",
            "cross-view clustering",
            "weighted aggregation",
            "semantic fusion",
            "geometric refinement",
            "densification"
          ],
          "githubStars": 1,
          "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
          }
        },
        "publishedAt": "2026-01-08T04:20:46.000Z",
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04754.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "695f7dcd6b694e775adbdffb",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lliCiWhG65vNHWooV1YFv.jpeg",
          "fullname": "Remi Chiou",
          "name": "remiii25",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63e39e6499a032b1c950403d",
          "name": "NYCU",
          "fullname": "National Yang Ming Chiao Tung University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04620",
          "authors": [
            {
              "_id": "696099de5b7998385e6395ad",
              "name": "Di Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T05:49:01.000Z",
          "submittedOnDailyAt": "2026-01-09T07:11:18.509Z",
          "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
          "submittedOnDailyBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
          "upvotes": 1,
          "discussionId": "696099df5b7998385e6395ae",
          "projectPage": "https://trotsky1997.github.io/agentdevel-dashboard/",
          "ai_summary": "AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.",
          "ai_keywords": [
            "large language model agents",
            "release engineering",
            "regression-aware release pipeline",
            "implementation-blind LLM critic",
            "executable diagnosis",
            "flip-centered gating",
            "canonical version line",
            "auditability",
            "reproducibility"
          ]
        },
        "publishedAt": "2026-01-08T00:49:01.000Z",
        "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
        "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04620.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64bce15bafd1e46c5504ad38",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
          "fullname": "Di Zhang",
          "name": "di-zhang-fdu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 166,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04300",
          "authors": [
            {
              "_id": "6960c0435b7998385e63967b",
              "name": "Chenye Meng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967c",
              "name": "Zejian Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967d",
              "name": "Zhongni Liu",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967e",
              "name": "Yize Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e63967f",
              "name": "Changle Xie",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639680",
              "name": "Kaixin Jia",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639681",
              "name": "Ling Yang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639682",
              "name": "Huanghuang Deng",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639683",
              "name": "Shiying Ding",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639684",
              "name": "Shengyuan Zhang",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639685",
              "name": "Jiayi Li",
              "hidden": false
            },
            {
              "_id": "6960c0435b7998385e639686",
              "name": "Lingyun Sun",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-07T18:11:22.000Z",
          "submittedOnDailyAt": "2026-01-09T06:20:51.665Z",
          "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
          "submittedOnDailyBy": {
            "_id": "66d1fb162e0412fa2aa4ffb0",
            "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
            "isPro": false,
            "fullname": "Chenye Meng",
            "user": "mengcy",
            "type": "user"
          },
          "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
          "upvotes": 2,
          "discussionId": "6960c0435b7998385e639687",
          "ai_summary": "A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.",
          "ai_keywords": [
            "diffusion models",
            "supervised fine-tuning",
            "complex preference optimization",
            "hierarchical evaluation criteria",
            "domain knowledge",
            "attribute decomposition",
            "preference optimization",
            "painting generation"
          ],
          "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
          }
        },
        "publishedAt": "2026-01-07T13:11:22.000Z",
        "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
        "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04300.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "66d1fb162e0412fa2aa4ffb0",
          "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
          "fullname": "Chenye Meng",
          "name": "mengcy",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61bac2af530e5c78d7b99667",
          "name": "zju",
          "fullname": "Zhejiang University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2512.24160",
          "authors": [
            {
              "_id": "6960b1f75b7998385e639626",
              "user": {
                "_id": "64c4c2291d44fc06afcf0801",
                "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
                "isPro": false,
                "fullname": "nina ni",
                "user": "water-fountain",
                "type": "user"
              },
              "name": "TsaiChing Ni",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:23:58.842Z",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639627",
              "name": "ZhenQi Chen",
              "hidden": false
            },
            {
              "_id": "6960b1f75b7998385e639628",
              "name": "YuanFu Yang",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-30T11:45:22.000Z",
          "submittedOnDailyAt": "2026-01-09T05:57:33.036Z",
          "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
          "submittedOnDailyBy": {
            "_id": "64c4c2291d44fc06afcf0801",
            "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
            "isPro": false,
            "fullname": "nina ni",
            "user": "water-fountain",
            "type": "user"
          },
          "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
          "upvotes": 2,
          "discussionId": "6960b1f75b7998385e639629",
          "projectPage": "https://ninaneon.github.io/projectpage/",
          "githubRepo": "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-",
          "githubRepoAddedBy": "user",
          "ai_summary": "A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.",
          "ai_keywords": [
            "Industrial Multimodal Defect Dataset",
            "vision-language foundation model",
            "diffusion-based model",
            "fine-grained textual descriptions",
            "multimodal learning",
            "domain-adaptive",
            "data-efficient foundation model adaptation"
          ],
          "githubStars": 2
        },
        "publishedAt": "2025-12-30T06:45:22.000Z",
        "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
        "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24160.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64c4c2291d44fc06afcf0801",
          "avatarUrl": "/avatars/bf7d7080993cd180871eb9e3842ea8ed.svg",
          "fullname": "nina ni",
          "name": "water-fountain",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05249",
          "authors": [
            {
              "_id": "6960a8ce5b7998385e639615",
              "user": {
                "_id": "676ce504027822ead2b5f193",
                "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg",
                "isPro": false,
                "fullname": "YuanKangNeilLee",
                "user": "NeilLeeNTU",
                "type": "user"
              },
              "name": "Yuan-Kang Lee",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:03.428Z",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639616",
              "name": "Kuan-Lin Chen",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639617",
              "name": "Chia-Che Chang",
              "hidden": false
            },
            {
              "_id": "6960a8ce5b7998385e639618",
              "user": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
              },
              "name": "Yu-Lun Liu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:50:20.802Z",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
          ],
          "publishedAt": "2026-01-08T18:59:55.000Z",
          "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
          "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
          "submittedOnDailyBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "isPro": false,
            "fullname": "Yu-Lun Liu",
            "user": "yulunliu",
            "type": "user"
          },
          "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
          "upvotes": 38,
          "discussionId": "6960a8ce5b7998385e639619",
          "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
          "githubRepo": "https://github.com/BrianChen1120/RL-AWB",
          "githubRepoAddedBy": "user",
          "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
          "ai_keywords": [
            "deep reinforcement learning",
            "color constancy",
            "white balance",
            "statistical algorithms",
            "illumination estimation",
            "multi-sensor dataset"
          ],
          "githubStars": 17
        },
        "publishedAt": "2026-01-08T13:59:55.000Z",
        "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6459d5da3b6fafd9664807ab",
          "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
          "fullname": "Yu-Lun Liu",
          "name": "yulunliu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 8,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.02016",
          "authors": [
            {
              "_id": "6960a4995b7998385e639609",
              "user": {
                "_id": "664ae70790135abe9ba76c28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
                "isPro": false,
                "fullname": "Matthias Bartolo",
                "user": "mbar0075",
                "type": "user"
              },
              "name": "Matthias Bartolo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:24:06.946Z",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960a",
              "name": "Dylan Seychell",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960b",
              "name": "Gabriel Hili",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960c",
              "name": "Matthew Montebello",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960d",
              "name": "Carl James Debono",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960e",
              "name": "Saviour Formosa",
              "hidden": false
            },
            {
              "_id": "6960a4995b7998385e63960f",
              "name": "Konstantinos Makantasis",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T11:24:34.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "submittedOnDailyBy": {
            "_id": "664ae70790135abe9ba76c28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
            "isPro": false,
            "fullname": "Matthias Bartolo",
            "user": "mbar0075",
            "type": "user"
          },
          "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
          "upvotes": 1,
          "discussionId": "6960a4995b7998385e639610",
          "githubRepo": "https://github.com/mbar0075/lupi-for-object-detection",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
          "ai_keywords": [
            "Learning Using Privileged Information",
            "object detection",
            "teacher-student architecture",
            "bounding box masks",
            "saliency maps",
            "depth cues",
            "model-agnostic methodology",
            "UAV-based litter detection",
            "Pascal VOC 2012",
            "ablation studies",
            "intermediate weighting"
          ],
          "githubStars": 4
        },
        "publishedAt": "2026-01-05T06:24:34.000Z",
        "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
        "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "664ae70790135abe9ba76c28",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
          "fullname": "Matthias Bartolo",
          "name": "mbar0075",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 2,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.04767",
          "authors": [
            {
              "_id": "69609fe35b7998385e6395ed",
              "user": {
                "_id": "64feba7efa64465422ce3003",
                "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
                "isPro": false,
                "fullname": "zongzefang",
                "user": "zzfoutofspace",
                "type": "user"
              },
              "name": "Zefang Zong",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:07.260Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ee",
              "user": {
                "_id": "6462271493f702673bf99c0b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg",
                "isPro": false,
                "fullname": "Dingwei Chen",
                "user": "CuSO4-Chen",
                "type": "user"
              },
              "name": "Dingwei Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:13.098Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395ef",
              "name": "Yang Li",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f0",
              "name": "Qi Yi",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f1",
              "name": "Bo Zhou",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f2",
              "user": {
                "_id": "65d5f457d032b44853ae79e4",
                "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg",
                "isPro": false,
                "fullname": "chengming li",
                "user": "daming8000",
                "type": "user"
              },
              "name": "Chengming Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:28.358Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f3",
              "name": "Bo Qian",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f4",
              "user": {
                "_id": "60799b15921db717010c7c8e",
                "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg",
                "isPro": false,
                "fullname": "Peng Chen",
                "user": "pengchen",
                "type": "user"
              },
              "name": "Peng Chen",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:54:22.694Z",
              "hidden": false
            },
            {
              "_id": "69609fe35b7998385e6395f5",
              "name": "Jie Jiang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T09:35:49.000Z",
          "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
          "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
          "submittedOnDailyBy": {
            "_id": "64feba7efa64465422ce3003",
            "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
            "isPro": false,
            "fullname": "zongzefang",
            "user": "zzfoutofspace",
            "type": "user"
          },
          "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
          "upvotes": 24,
          "discussionId": "69609fe35b7998385e6395f6",
          "githubRepo": "https://github.com/zzfoutofspace/ATPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
          "ai_keywords": [
            "Agentic Reinforcement Learning",
            "tree search",
            "Entropy-Guided Tree Expansion",
            "Turn-wise Credit Assignment",
            "Agentic Turn-based Policy Optimization",
            "multi-turn tasks",
            "policy optimization",
            "reward propagation",
            "turn-level learning objective"
          ],
          "githubStars": 6,
          "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
          }
        },
        "publishedAt": "2026-01-08T04:35:49.000Z",
        "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
        "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "64feba7efa64465422ce3003",
          "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
          "fullname": "zongzefang",
          "name": "zzfoutofspace",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "66543b6e420092799d2f625c",
          "name": "tencent",
          "fullname": "Tencent",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03362",
          "authors": [
            {
              "_id": "695fa6e22450f142afb3c507",
              "user": {
                "_id": "657dc1576dc01435cd9029d8",
                "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                "isPro": false,
                "fullname": "Xiang Zhang",
                "user": "XiangZ",
                "type": "user"
              },
              "name": "Xiang Zhang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:36:00.258Z",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c508",
              "name": "Yang Zhang",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c509",
              "name": "Lukas Mehl",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50a",
              "name": "Markus Gross",
              "hidden": false
            },
            {
              "_id": "695fa6e22450f142afb3c50b",
              "name": "Christopher Schroers",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T19:02:34.000Z",
          "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "submittedOnDailyBy": {
            "_id": "657dc1576dc01435cd9029d8",
            "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "XiangZ",
            "type": "user"
          },
          "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
          "upvotes": 2,
          "discussionId": "695fa6e22450f142afb3c50c",
          "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
          "ai_keywords": [
            "depth fixer network",
            "gated residual module",
            "depth-based forward warping",
            "generative scene painter",
            "color fuser",
            "monocular depth estimation",
            "stereo image conversion",
            "novel view synthesis"
          ],
          "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
          }
        },
        "publishedAt": "2026-01-06T14:02:34.000Z",
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "657dc1576dc01435cd9029d8",
          "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
          "fullname": "Xiang Zhang",
          "name": "XiangZ",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "63263d7db8e57aab1a778773",
          "name": "ethz",
          "fullname": "ETH Zurich",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05124",
          "authors": [
            {
              "_id": "69608d7e5b7998385e639573",
              "user": {
                "_id": "64623cc1514ee1645bcfaecd",
                "avatarUrl": "/avatars/f315de774325064bc78677f6d134484f.svg",
                "isPro": false,
                "fullname": "hrz",
                "user": "hrz2000",
                "type": "user"
              },
              "name": "Runze He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:51.987Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639574",
              "name": "Yiji Cheng",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639575",
              "name": "Tiankai Hang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639576",
              "user": {
                "_id": "6645835a2b57c619a19cc0c4",
                "avatarUrl": "/avatars/a058b27b2452c4315c9c542daaf8bb93.svg",
                "isPro": false,
                "fullname": "Zhiminli",
                "user": "Zhiminli",
                "type": "user"
              },
              "name": "Zhimin Li",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:18.656Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639577",
              "name": "Yu Xu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639578",
              "name": "Zijin Yin",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639579",
              "name": "Shiyi Zhang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957a",
              "name": "Wenxun Dai",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957b",
              "user": {
                "_id": "647076467fd7ecdbd0ea03b1",
                "avatarUrl": "/avatars/6e090ea5f88977c6f70544175094c2a6.svg",
                "isPro": false,
                "fullname": "Penghui Du",
                "user": "eternaldolphin",
                "type": "user"
              },
              "name": "Penghui Du",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T16:51:09.364Z",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957c",
              "name": "Ao Ma",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957d",
              "name": "Chunyu Wang",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957e",
              "name": "Qinglin Lu",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e63957f",
              "name": "Jizhong Han",
              "hidden": false
            },
            {
              "_id": "69608d7e5b7998385e639580",
              "name": "Jiao Dai",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:13:00.000Z",
          "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
          "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
          "submittedOnDailyBy": {
            "_id": "6428fd124fe87caede856311",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
            "isPro": false,
            "fullname": "Xianghao Kong",
            "user": "refkxh",
            "type": "user"
          },
          "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
          "upvotes": 5,
          "discussionId": "69608d7f5b7998385e639581",
          "projectPage": "https://hrz2000.github.io/realign/",
          "githubRepo": "https://github.com/hrz2000/realign",
          "githubRepoAddedBy": "user",
          "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
          "ai_keywords": [
            "In-Context Chain-of-Thought",
            "structured reasoning",
            "reinforcement learning",
            "surrogate reward",
            "multimodal models",
            "image generation",
            "image editing"
          ],
          "githubStars": 1
        },
        "publishedAt": "2026-01-08T12:13:00.000Z",
        "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
        "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "6428fd124fe87caede856311",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
          "fullname": "Xianghao Kong",
          "name": "refkxh",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.04890",
          "authors": [
            {
              "_id": "69608e7c5b7998385e639583",
              "user": {
                "_id": "64670db15993aa7666cc6022",
                "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                "isPro": false,
                "fullname": "Maksim Velikanov",
                "user": "yellowvm",
                "type": "user"
              },
              "name": "Maksim Velikanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:44.975Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639584",
              "user": {
                "_id": "6697a9fb6d173ec7382e0392",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg",
                "isPro": false,
                "fullname": "Ilyas Chahed",
                "user": "IChahed",
                "type": "user"
              },
              "name": "Ilyas Chahed",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:04.314Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639585",
              "user": {
                "_id": "6460c3811db65f878513bcaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                "isPro": false,
                "fullname": "Jingwei Zuo",
                "user": "JingweiZuo",
                "type": "user"
              },
              "name": "Jingwei Zuo",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:49.874Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639586",
              "name": "Dhia Eddine Rhaiem",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639587",
              "user": {
                "_id": "62441d1d9fdefb55a0b7d12c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                "isPro": false,
                "fullname": "Younes B",
                "user": "ybelkada",
                "type": "user"
              },
              "name": "Younes Belkada",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:34:47.914Z",
              "hidden": false
            },
            {
              "_id": "69608e7c5b7998385e639588",
              "user": {
                "_id": "6471d727a2b0a376b8b6a4ed",
                "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg",
                "isPro": false,
                "fullname": "Hakim Hacid",
                "user": "HakimHacid",
                "type": "user"
              },
              "name": "Hakim Hacid",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:56.651Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T12:41:49.000Z",
          "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
          "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
          "submittedOnDailyBy": {
            "_id": "6460c3811db65f878513bcaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
            "isPro": false,
            "fullname": "Jingwei Zuo",
            "user": "JingweiZuo",
            "type": "user"
          },
          "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
          "upvotes": 39,
          "discussionId": "69608e7c5b7998385e639589",
          "projectPage": "https://tiiuae.github.io/Falcon-H1/",
          "githubRepo": "https://github.com/tiiuae/falcon-h1",
          "githubRepoAddedBy": "user",
          "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
          "ai_keywords": [
            "weight decay",
            "stochastic gradient noise",
            "Brownian-like expansion",
            "WD-noise equilibrium",
            "learnable multipliers",
            "matrix layers",
            "weight norm",
            "muP multipliers",
            "Adam optimizer",
            "Muon optimizer"
          ],
          "githubStars": 99,
          "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
          }
        },
        "publishedAt": "2026-01-08T07:41:49.000Z",
        "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
        "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6460c3811db65f878513bcaf",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
          "fullname": "Jingwei Zuo",
          "name": "JingweiZuo",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 38,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6448cad23adf50d86406b0a3",
          "name": "tiiuae",
          "fullname": "Technology Innovation Institute",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05241",
          "authors": [
            {
              "_id": "6960775a5b7998385e6394ff",
              "user": {
                "_id": "64ed876a74d9b58eabc769a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
                "isPro": true,
                "fullname": "Boyang Wang",
                "user": "HikariDawn",
                "type": "user"
              },
              "name": "Boyang Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:11.481Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639500",
              "name": "Haoran Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639501",
              "name": "Shujie Zhang",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639502",
              "user": {
                "_id": "64edb581067fbb625f893628",
                "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg",
                "isPro": false,
                "fullname": "hao",
                "user": "wuzhi-hao",
                "type": "user"
              },
              "name": "Jinkun Hao",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:09.450Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639503",
              "name": "Mingda Jia",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639504",
              "name": "Qi Lv",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639505",
              "user": {
                "_id": "65de9c6cf68c3d3bac330509",
                "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                "isPro": false,
                "fullname": "Yucheng Mao",
                "user": "matthewmao",
                "type": "user"
              },
              "name": "Yucheng Mao",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:47.293Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639506",
              "user": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
              },
              "name": "Zhaoyang Lyu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:53.025Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639507",
              "user": {
                "_id": "685d08b9fc7a0ff2f338dbd0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png",
                "isPro": false,
                "fullname": "Jia Zeng",
                "user": "Jia-Zeng",
                "type": "user"
              },
              "name": "Jia Zeng",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T15:46:06.251Z",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639508",
              "name": "Xudong Xu",
              "hidden": false
            },
            {
              "_id": "6960775a5b7998385e639509",
              "user": {
                "_id": "65783ee6ee33d547aecc3ffc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                "isPro": false,
                "fullname": "Jiangmiao Pang",
                "user": "Jiangmiao",
                "type": "user"
              },
              "name": "Jiangmiao Pang",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:52:00.637Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:22.000Z",
          "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
          "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
          "submittedOnDailyBy": {
            "_id": "64ed876a74d9b58eabc769a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
            "isPro": true,
            "fullname": "Boyang Wang",
            "user": "HikariDawn",
            "type": "user"
          },
          "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
          "upvotes": 22,
          "discussionId": "6960775a5b7998385e63950a",
          "projectPage": "https://robovip.github.io/RoboVIP/",
          "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
          "githubRepoAddedBy": "user",
          "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
          "ai_keywords": [
            "image diffusion models",
            "visual identity prompting",
            "manipulation data",
            "vision-language-action models",
            "visuomotor policy models",
            "visual identity pool"
          ],
          "githubStars": 13
        },
        "publishedAt": "2026-01-08T13:59:22.000Z",
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
        "numComments": 3,
        "submittedBy": {
          "_id": "64ed876a74d9b58eabc769a4",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
          "fullname": "Boyang Wang",
          "name": "HikariDawn",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 12,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2512.23628",
          "authors": [
            {
              "_id": "6960865d5b7998385e639560",
              "name": "Shu Pu",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639561",
              "name": "Boya Zeng",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639562",
              "name": "Kaichen Zhou",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639563",
              "name": "Mengyu Wang",
              "hidden": false
            },
            {
              "_id": "6960865d5b7998385e639564",
              "name": "Zhuang Liu",
              "hidden": false
            }
          ],
          "publishedAt": "2025-12-29T17:39:21.000Z",
          "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
          "title": "Memorization in 3D Shape Generation: An Empirical Study",
          "submittedOnDailyBy": {
            "_id": "663ba53b96c98cfb8ded1c4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
            "isPro": true,
            "fullname": "Shu Pu",
            "user": "pudashi",
            "type": "user"
          },
          "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
          "upvotes": 2,
          "discussionId": "6960865e5b7998385e639565",
          "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
          "ai_keywords": [
            "generative models",
            "3D vision",
            "memorization",
            "latent vector-set",
            "diffusion model",
            "data modality",
            "data diversity",
            "conditioning",
            "guidance scale",
            "Vecset",
            "rotation augmentation"
          ],
          "organization": {
            "_id": "6735d51c08a190b1caea1f29",
            "name": "PrincetonUniversity",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
          }
        },
        "publishedAt": "2025-12-29T12:39:21.000Z",
        "title": "Memorization in 3D Shape Generation: An Empirical Study",
        "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "663ba53b96c98cfb8ded1c4f",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
          "fullname": "Shu Pu",
          "name": "pudashi",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 1,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "6735d51c08a190b1caea1f29",
          "name": "PrincetonUniversity",
          "fullname": "Princeton University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05239",
          "authors": [
            {
              "_id": "69607a775b7998385e639541",
              "name": "Xiao Fu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639542",
              "name": "Shitao Tang",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639543",
              "name": "Min Shi",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639544",
              "name": "Xian Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639545",
              "name": "Jinwei Gu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639546",
              "name": "Ming-Yu Liu",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639547",
              "name": "Dahua Lin",
              "hidden": false
            },
            {
              "_id": "69607a775b7998385e639548",
              "name": "Chen-Hsuan Lin",
              "hidden": false
            }
          ],
          "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
          ],
          "publishedAt": "2026-01-08T18:58:32.000Z",
          "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
          "title": "Plenoptic Video Generation",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "upvotes": 9,
          "discussionId": "69607a775b7998385e639549",
          "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
          "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
          "ai_keywords": [
            "generative video re-rendering",
            "spatio-temporal coherence",
            "autoregressive training",
            "camera-guided video retrieval",
            "progressive context-scaling",
            "self-conditioning",
            "long-video conditioning",
            "multi-in-single-out model",
            "video-conditioned model"
          ],
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:58:32.000Z",
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "mediaUrls": [
          "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05242",
          "authors": [
            {
              "_id": "69607a225b7998385e63952a",
              "user": {
                "_id": "62b58c68a1bae3c711c41321",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
                "isPro": false,
                "fullname": "LIU Shih-yang",
                "user": "sliuau",
                "type": "user"
              },
              "name": "Shih-Yang Liu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:01.190Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952b",
              "name": "Xin Dong",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952c",
              "user": {
                "_id": "640928bd3461c51cf7378707",
                "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg",
                "isPro": false,
                "fullname": "Ximing Lu",
                "user": "Ximing",
                "type": "user"
              },
              "name": "Ximing Lu",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T08:49:57.401Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952d",
              "name": "Shizhe Diao",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952e",
              "user": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
              },
              "name": "Peter Belcak",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:49:07.360Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e63952f",
              "name": "Mingjie Liu",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639530",
              "user": {
                "_id": "64ae22dd1aee69ece065cdcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                "isPro": false,
                "fullname": "Min-Hung Chen",
                "user": "cmhungsteve",
                "type": "user"
              },
              "name": "Min-Hung Chen",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:03.130Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639531",
              "user": {
                "_id": "65a8b7f69aec1645994e7a15",
                "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                "isPro": false,
                "fullname": "Hongxu Yin",
                "user": "yinhongxu",
                "type": "user"
              },
              "name": "Hongxu Yin",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:57.052Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639532",
              "name": "Yu-Chiang Frank Wang",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639533",
              "name": "Kwang-Ting Cheng",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639534",
              "user": {
                "_id": "64d42729f63b01b7f676b176",
                "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                "isPro": false,
                "fullname": "Yejin Choi",
                "user": "yejinchoinka",
                "type": "user"
              },
              "name": "Yejin Choi",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:43.597Z",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639535",
              "name": "Jan Kautz",
              "hidden": false
            },
            {
              "_id": "69607a225b7998385e639536",
              "user": {
                "_id": "646d0c1c534e52f8c30500a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png",
                "isPro": true,
                "fullname": "Pavlo Molchanov",
                "user": "pmolchanov",
                "type": "user"
              },
              "name": "Pavlo Molchanov",
              "status": "admin_assigned",
              "statusLastChangedAt": "2026-01-09T15:48:21.861Z",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T18:59:24.000Z",
          "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
          "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
          "submittedOnDailyBy": {
            "_id": "62b58c68a1bae3c711c41321",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
            "isPro": false,
            "fullname": "LIU Shih-yang",
            "user": "sliuau",
            "type": "user"
          },
          "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
          "upvotes": 149,
          "discussionId": "69607a225b7998385e639537",
          "projectPage": "https://nvlabs.github.io/GDPO/",
          "githubRepo": "https://github.com/NVlabs/GDPO",
          "githubRepoAddedBy": "user",
          "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
          "ai_keywords": [
            "Reinforcement learning",
            "Group Relative Policy Optimization",
            "multi-reward setting",
            "policy optimization",
            "Group reward-Decoupled Normalization Policy Optimization",
            "reward normalization",
            "advantage values",
            "training stability",
            "multi-reward reinforcement learning"
          ],
          "githubStars": 126,
          "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
          }
        },
        "publishedAt": "2026-01-08T13:59:24.000Z",
        "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
        "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
        "numComments": 6,
        "submittedBy": {
          "_id": "62b58c68a1bae3c711c41321",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
          "fullname": "LIU Shih-yang",
          "name": "sliuau",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "60262b67268c201cdc8b7d43",
          "name": "nvidia",
          "fullname": "NVIDIA",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.01887",
          "authors": [
            {
              "_id": "696077705b7998385e63950c",
              "name": "Jiawen Zhang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950d",
              "user": {
                "_id": "6433707307bad11484af1d2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                "isPro": false,
                "fullname": "Lipeng (Tony) He",
                "user": "ttttonyhe",
                "type": "user"
              },
              "name": "Lipeng He",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:07.378Z",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950e",
              "name": "Kejia Chen",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e63950f",
              "name": "Jian Lou",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639510",
              "name": "Jian Liu",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639511",
              "name": "Xiaohu Yang",
              "hidden": false
            },
            {
              "_id": "696077705b7998385e639512",
              "name": "Ruoxi Jia",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-05T08:26:34.000Z",
          "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
          "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
          "submittedOnDailyBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "isPro": false,
            "fullname": "Lipeng (Tony) He",
            "user": "ttttonyhe",
            "type": "user"
          },
          "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
          "upvotes": 0,
          "discussionId": "696077705b7998385e639513",
          "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
          "ai_keywords": [
            "large language models",
            "fine-tuning",
            "safety alignment",
            "harmful examples",
            "low-rank structure",
            "safety gradient",
            "convergence"
          ]
        },
        "publishedAt": "2026-01-05T03:26:34.000Z",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "6433707307bad11484af1d2a",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
          "fullname": "Lipeng (Tony) He",
          "name": "ttttonyhe",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.05106",
          "authors": [
            {
              "_id": "696073d35b7998385e6394c4",
              "name": "Nuoya Xiong",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c5",
              "user": {
                "_id": "64887eb15cf73a16e767b56a",
                "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg",
                "isPro": false,
                "fullname": "Yuhang Zhou",
                "user": "zyhang1998",
                "type": "user"
              },
              "name": "Yuhang Zhou",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T16:50:49.484Z",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c6",
              "name": "Hanqing Zeng",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c7",
              "name": "Zhaorun Chen",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c8",
              "name": "Furong Huang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394c9",
              "name": "Shuchao Bi",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394ca",
              "name": "Lizhu Zhang",
              "hidden": false
            },
            {
              "_id": "696073d35b7998385e6394cb",
              "name": "Zhuokai Zhao",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T16:53:16.000Z",
          "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
          "title": "Token-Level LLM Collaboration via FusionRoute",
          "submittedOnDailyBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
          "upvotes": 34,
          "discussionId": "696073d45b7998385e6394cc",
          "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
          "ai_keywords": [
            "large language models",
            "token-level collaboration",
            "multi-LLM collaboration",
            "lightweight router",
            "expert selection",
            "logit addition",
            "complementary generator",
            "optimal decoding policy",
            "model merging",
            "direct fine-tuning"
          ]
        },
        "publishedAt": "2026-01-08T11:53:16.000Z",
        "title": "Token-Level LLM Collaboration via FusionRoute",
        "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
        "numComments": 1,
        "submittedBy": {
          "_id": "6039478ab3ecf716b1a5fd4d",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
          "fullname": "taesiri",
          "name": "taesiri",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 206,
          "isUserFollowing": false
        },
        "isAuthorParticipating": false
      },
      {
        "paper": {
          "id": "2601.05163",
          "authors": [
            {
              "_id": "696071945b7998385e6394b7",
              "name": "Qintong Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b8",
              "name": "Xinjie Lv",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394b9",
              "user": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
              },
              "name": "Jialong Wu",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:13.405Z",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394ba",
              "name": "Baixuan Li",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bb",
              "name": "Zhengwei Tao",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bc",
              "name": "Guochen Yan",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bd",
              "name": "Huanyao Zhang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394be",
              "name": "Bin Wang",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394bf",
              "name": "Jiahao Xu",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c0",
              "name": "Haitao Mi",
              "hidden": false
            },
            {
              "_id": "696071945b7998385e6394c1",
              "name": "Wentao Zhang",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-08T17:54:32.000Z",
          "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
          "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
          "submittedOnDailyBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
          "upvotes": 4,
          "discussionId": "696071945b7998385e6394c2",
          "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
          "ai_keywords": [
            "document question answering",
            "information-seeking problem",
            "tool-driven agent framework",
            "document exploration",
            "document comprehension",
            "end-to-end training",
            "data synthesis pipeline",
            "long-context document understanding",
            "MMLongBench-Doc",
            "DocBench"
          ],
          "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
          }
        },
        "publishedAt": "2026-01-08T12:54:32.000Z",
        "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
        "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "644a4fbc2166258fccc664bc",
          "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
          "fullname": "Jialong Wu",
          "name": "callanwu",
          "type": "user",
          "isPro": false,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 32,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "61dcd8e344f59573371b5cb6",
          "name": "PekingUniversity",
          "fullname": "Peking University",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
      },
      {
        "paper": {
          "id": "2601.03425",
          "authors": [
            {
              "_id": "69606e2d5b7998385e6394a1",
              "user": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
              },
              "name": "Yan Wang",
              "status": "claimed_verified",
              "statusLastChangedAt": "2026-01-09T08:35:15.313Z",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a2",
              "name": "Yitao Xu",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a3",
              "name": "Nanhan Shen",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a4",
              "name": "Jinyan Su",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a5",
              "name": "Jimin Huang",
              "hidden": false
            },
            {
              "_id": "69606e2d5b7998385e6394a6",
              "name": "Zining Zhu",
              "hidden": false
            }
          ],
          "publishedAt": "2026-01-06T21:29:45.000Z",
          "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
          "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
          "submittedOnDailyBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
          "upvotes": 13,
          "discussionId": "69606e2d5b7998385e6394a7",
          "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
          "ai_keywords": [
            "Mixture of Experts",
            "sparse routing",
            "COMMITTEEAUDIT",
            "domain specialization",
            "Standing Committee",
            "expert groups",
            "routing behavior",
            "MMLU benchmark",
            "load-balancing losses",
            "expert utilization"
          ],
          "organization": {
            "_id": "658f4413674349122c0708e9",
            "name": "TheFinAI",
            "fullname": "The Fin AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
          }
        },
        "publishedAt": "2026-01-06T16:29:45.000Z",
        "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
        "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
        "numComments": 2,
        "submittedBy": {
          "_id": "65d76cc5b9b7b8bf88faa916",
          "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
          "fullname": "Yan Wang",
          "name": "YanAdjeNole",
          "type": "user",
          "isPro": true,
          "isHf": false,
          "isHfAdmin": false,
          "isMod": false,
          "followerCount": 3,
          "isUserFollowing": false
        },
        "organization": {
          "_id": "658f4413674349122c0708e9",
          "name": "TheFinAI",
          "fullname": "The Fin AI",
          "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
        },
        "isAuthorParticipating": true
      }
    ]
  },
  "metadata": {
    "total num": {
      "2025/01": 50,
      "2025/02": 50,
      "2025/03": 50,
      "2025/04": 50,
      "2025/05": 50,
      "2025/06": 50,
      "2025/07": 50,
      "2025/08": 50,
      "2025/09": 50,
      "2025/10": 50,
      "2025/11": 50,
      "2025/12": 50
    }
  }
}