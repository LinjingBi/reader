
==========================================================================================
# month=2025-01 BEST CLUSTERING (mode=B, k=5)

------------------------------------------------------------------------------------------
Cluster 2 | size=24

[2501.12599] Kimi k1.5: Scaling Reinforcement Learning with LLMs
URL: https://huggingface.co/papers/2501.12599
Summary: Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by l…

[2501.11425] Agent-R: Training Language Model Agents to Reflect via Iterative
  Self-Training
URL: https://huggingface.co/papers/2501.11425
Summary: Large Language Models (LLMs) agents are increasingly pivotal for addressing
complex tasks in interactive environments. Existing work mainly focuses on
enhancing performance through behavior cloning from stronger experts, yet such
approaches often falter in real-world applications, mainly due to the inability
to recover from errors. However, step-le…

[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
  Reinforcement Learning
URL: https://huggingface.co/papers/2501.12948
Summary: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and
DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement
learning (RL) without supervised fine-tuning (SFT) as a preliminary step,
demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero
naturally emerges with numerous powerful and intr…

[2501.12895] Test-Time Preference Optimization: On-the-Fly Alignment via Iterative
  Textual Feedback
URL: https://huggingface.co/papers/2501.12895
Summary: Large language models (LLMs) demonstrate impressive performance but lack the
flexibility to adapt to human preferences quickly without retraining. In this
work, we introduce Test-time Preference Optimization (TPO), a framework that
aligns LLM outputs with human preferences during inference, removing the need
to update model parameters. Rather than …

[2501.15383] Qwen2.5-1M Technical Report
URL: https://huggingface.co/papers/2501.15383
Summary: We introduce Qwen2.5-1M, a series of models that extend the context length to
1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series
have significantly enhanced long-context capabilities through long-context
pre-training and post-training. Key techniques such as long data synthesis,
progressive pre-training, and multi-stage …

[2501.17161] SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model
  Post-training
URL: https://huggingface.co/papers/2501.17161
Summary: Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used
post-training techniques for foundation models. However, their roles in
enhancing model generalization capabilities remain unclear. This paper studies
the difference between SFT and RL on generalization and memorization, focusing
on text-based rule variants and visual vari…

[2501.05032] Enhancing Human-Like Responses in Large Language Models
URL: https://huggingface.co/papers/2501.05032
Summary: This paper explores the advancements in making large language models (LLMs)
more human-like. We focus on techniques that enhance natural language
understanding, conversational coherence, and emotional intelligence in AI
systems. The study evaluates various approaches, including fine-tuning with
diverse datasets, incorporating psychological principl…

[2501.08313] MiniMax-01: Scaling Foundation Models with Lightning Attention
URL: https://huggingface.co/papers/2501.08313
Summary: We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,
which are comparable to top-tier models while offering superior capabilities in
processing longer contexts. The core lies in lightning attention and its
efficient scaling. To maximize computational capacity, we integrate it with
Mixture of Experts (MoE), creating a model w…

[2501.09891] Evolving Deeper LLM Thinking
URL: https://huggingface.co/papers/2501.09891
Summary: We explore an evolutionary search strategy for scaling inference time compute
in Large Language Models. The proposed approach, Mind Evolution, uses a
language model to generate, recombine and refine candidate responses. The
proposed approach avoids the need to formalize the underlying inference problem
whenever a solution evaluator is available. Co…

[2501.06252] Transformer^2: Self-adaptive LLMs
URL: https://huggingface.co/papers/2501.06252
Summary: Self-adaptive large language models (LLMs) aim to solve the challenges posed
by traditional fine-tuning methods, which are often computationally intensive
and static in their ability to handle diverse tasks. We introduce \implname, a
novel self-adaptation framework that adapts LLMs for unseen tasks in real-time
by selectively adjusting only the sin…

[2501.07301] The Lessons of Developing Process Reward Models in Mathematical
  Reasoning
URL: https://huggingface.co/papers/2501.07301
Summary: Process Reward Models (PRMs) emerge as a promising approach for process
supervision in mathematical reasoning of Large Language Models (LLMs), which
aim to identify and mitigate intermediate errors in the reasoning processes.
However, the development of effective PRMs faces significant challenges,
particularly in data annotation and evaluation meth…

[2501.10120] PaSa: An LLM Agent for Comprehensive Academic Paper Search
URL: https://huggingface.co/papers/2501.10120
Summary: We introduce PaSa, an advanced Paper Search agent powered by large language
models. PaSa can autonomously make a series of decisions, including invoking
search tools, reading papers, and selecting relevant references, to ultimately
obtain comprehensive and accurate results for complex scholarly queries. We
optimize PaSa using reinforcement learning…

[2501.04682] Towards System 2 Reasoning in LLMs: Learning How to Think With Meta
  Chain-of-Though
URL: https://huggingface.co/papers/2501.04682
Summary: We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends
traditional Chain-of-Thought (CoT) by explicitly modeling the underlying
reasoning required to arrive at a particular CoT. We present empirical evidence
from state-of-the-art models exhibiting behaviors consistent with in-context
search, and explore methods for producing …

[2501.03262] REINFORCE++: A Simple and Efficient Approach for Aligning Large Language
  Models
URL: https://huggingface.co/papers/2501.03262
Summary: Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical
approach for aligning large language models with human preferences, witnessing
rapid algorithmic evolution through methods such as Proximal Policy
Optimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave
One-Out (RLOO), ReMax, and Group Relative Policy Opti…

[2501.05366] Search-o1: Agentic Search-Enhanced Large Reasoning Models
URL: https://huggingface.co/papers/2501.05366
Summary: Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive
long stepwise reasoning capabilities through large-scale reinforcement
learning. However, their extended reasoning processes often suffer from
knowledge insufficiency, leading to frequent uncertainties and potential
errors. To address this limitation, we introduce Search-o1, …

[2501.04519] rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep
  Thinking
URL: https://huggingface.co/papers/2501.04519
Summary: We present rStar-Math to demonstrate that small language models (SLMs) can
rival or even surpass the math reasoning capability of OpenAI o1, without
distillation from superior models. rStar-Math achieves this by exercising "deep
thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM
performs test-time search guided by an SLM-base…

[2501.11873] Demons in the Detail: On Implementing Load Balancing Loss for Training
  Specialized Mixture-of-Expert Models
URL: https://huggingface.co/papers/2501.11873
Summary: This paper revisits the implementation of
Load-balancing Loss (LBL) when training
Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E
sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i
represents the frequency of expert i being selected, and p_i denotes the
average gating score of the expert i. Existin…

[2501.13200] SRMT: Shared Memory for Multi-agent Lifelong Pathfinding
URL: https://huggingface.co/papers/2501.13200
Summary: Multi-agent reinforcement learning (MARL) demonstrates significant progress
in solving cooperative and competitive multi-agent problems in various
environments. One of the principal challenges in MARL is the need for explicit
prediction of the agents' behavior to achieve cooperation. To resolve this
issue, we propose the Shared Recurrent Memory Tra…

[2501.06282] MinMo: A Multimodal Large Language Model for Seamless Voice Interaction
URL: https://huggingface.co/papers/2501.06282
Summary: Recent advancements in large language models (LLMs) and multimodal
speech-text models have laid the groundwork for seamless voice interactions,
enabling real-time, natural, and human-like conversations. Previous models for
voice interactions are categorized as native and aligned. Native models
integrate speech and text processing in one framework b…

[2501.09732] Inference-Time Scaling for Diffusion Models beyond Scaling Denoising
  Steps
URL: https://huggingface.co/papers/2501.09732
Summary: Generative models have made significant impacts across various domains,
largely due to their ability to scale during training by increasing data,
computational resources, and model size, a phenomenon characterized by the
scaling laws. Recent research has begun to explore inference-time scaling
behavior in Large Language Models (LLMs), revealing how…

[2501.06425] Tensor Product Attention Is All You Need
URL: https://huggingface.co/papers/2501.06425
Summary: Scaling language models to handle longer input sequences typically
necessitates large key-value (KV) caches, resulting in substantial memory
overhead during inference. In this paper, we propose Tensor Product Attention
(TPA), a novel attention mechanism that uses tensor decompositions to represent
queries, keys, and values compactly, significantly …

[2501.18492] GuardReasoner: Towards Reasoning-based LLM Safeguards
URL: https://huggingface.co/papers/2501.18492
Summary: As LLMs increasingly impact safety-critical applications, ensuring their
safety using guardrails remains a key challenge. This paper proposes
GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to
reason. Concretely, we first create the GuardReasonerTrain dataset, which
consists of 127K samples with 460K detailed reasoning …

[2501.08365] Towards Best Practices for Open Datasets for LLM Training
URL: https://huggingface.co/papers/2501.08365
Summary: Many AI companies are training their large language models (LLMs) on data
without the permission of the copyright owners. The permissibility of doing so
varies by jurisdiction: in countries like the EU and Japan, this is allowed
under certain restrictions, while in the United States, the legal landscape is
more ambiguous. Regardless of the legal st…

[2501.03575] Cosmos World Foundation Model Platform for Physical AI
URL: https://huggingface.co/papers/2501.03575
Summary: Physical AI needs to be trained digitally first. It needs a digital twin of
itself, the policy model, and a digital twin of the world, the world model. In
this paper, we present the Cosmos World Foundation Model Platform to help
developers build customized world models for their Physical AI setups. We
position a world foundation model as a general-…

------------------------------------------------------------------------------------------
Cluster 0 | size=12

[2501.00958] 2.5 Years in Class: A Multimodal Textbook for Vision-Language
  Pretraining
URL: https://huggingface.co/papers/2501.00958
Summary: Compared to image-text pair data, interleaved corpora enable Vision-Language
Models (VLMs) to understand the world more naturally like humans. However, such
existing datasets are crawled from webpage, facing challenges like low
knowledge density, loose image-text relations, and poor logical coherence
between images. On the other hand, the internet …

[2501.13106] VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video
  Understanding
URL: https://huggingface.co/papers/2501.13106
Summary: In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation
model for image and video understanding. The core design philosophy of
VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the
vision-centric training paradigm and vision-centric framework design. The key
insight of our vision-centric training parad…

[2501.06186] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs
URL: https://huggingface.co/papers/2501.06186
Summary: Reasoning is a fundamental capability for solving complex multi-step
problems, particularly in visual contexts where sequential step-wise
understanding is essential. Existing approaches lack a comprehensive framework
for evaluating visual reasoning and do not emphasize step-wise problem-solving.
To this end, we propose a comprehensive framework for…

[2501.03895] LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One
  Vision Token
URL: https://huggingface.co/papers/2501.03895
Summary: The advent of real-time large multimodal models (LMMs) like GPT-4o has
sparked considerable interest in efficient LMMs. LMM frameworks typically
encode visual inputs into vision tokens (continuous representations) and
integrate them and textual instructions into the context of large language
models (LLMs), where large-scale parameters and numerous …

[2501.12380] MMVU: Measuring Expert-Level Multi-Discipline Video Understanding
URL: https://huggingface.co/papers/2501.12380
Summary: We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark
for evaluating foundation models in video understanding. MMVU includes 3,000
expert-annotated questions spanning 27 subjects across four core disciplines:
Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to
prior benchmarks, MMVU features three k…

[2501.05874] VideoRAG: Retrieval-Augmented Generation over Video Corpus
URL: https://huggingface.co/papers/2501.05874
Summary: Retrieval-Augmented Generation (RAG) is a powerful strategy to address the
issue of generating factually incorrect outputs in foundation models by
retrieving external knowledge relevant to queries and incorporating it into
their generation process. However, existing RAG approaches have primarily
focused on textual information, with some recent adva…

[2501.07171] BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and
  Vision-Language Models Derived from Scientific Literature
URL: https://huggingface.co/papers/2501.07171
Summary: The development of vision-language models (VLMs) is driven by large-scale and
diverse multimodal datasets. However, progress toward generalist biomedical
VLMs is limited by the lack of annotated, publicly accessible datasets across
biology and medicine. Existing efforts are restricted to narrow domains,
missing the full diversity of biomedical know…

[2501.03841] OmniManip: Towards General Robotic Manipulation via Object-Centric
  Interaction Primitives as Spatial Constraints
URL: https://huggingface.co/papers/2501.03841
Summary: The development of general robotic systems capable of manipulating in
unstructured environments is a significant challenge. While Vision-Language
Models(VLM) excel in high-level commonsense reasoning, they lack the
fine-grained 3D spatial understanding required for precise manipulation tasks.
Fine-tuning VLM on robotic datasets to create Vision-Lan…

[2501.12909] FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in
  Virtual 3D Spaces
URL: https://huggingface.co/papers/2501.12909
Summary: Virtual film production requires intricate decision-making processes,
including scriptwriting, virtual cinematography, and precise actor positioning
and actions. Motivated by recent advances in automated decision-making with
language agent-based societies, this paper introduces FilmAgent, a novel
LLM-based multi-agent collaborative framework for en…

[2501.01427] VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion
  Control
URL: https://huggingface.co/papers/2501.01427
Summary: Despite significant advancements in video generation, inserting a given
object into videos remains a challenging task. The difficulty lies in
preserving the appearance details of the reference object and accurately
modeling coherent motions at the same time. In this paper, we propose
VideoAnydoor, a zero-shot video object insertion framework with h…

[2501.12326] UI-TARS: Pioneering Automated GUI Interaction with Native Agents
URL: https://huggingface.co/papers/2501.12326
Summary: This paper introduces UI-TARS, a native GUI agent model that solely perceives
the screenshots as input and performs human-like interactions (e.g., keyboard
and mouse operations). Unlike prevailing agent frameworks that depend on
heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts
and workflows, UI-TARS is an end-to-end mode…

[2501.15368] Baichuan-Omni-1.5 Technical Report
URL: https://huggingface.co/papers/2501.15368
Summary: We introduce Baichuan-Omni-1.5, an omni-modal model that not only has
omni-modal understanding capabilities but also provides end-to-end audio
generation capabilities. To achieve fluent and high-quality interaction across
modalities without compromising the capabilities of any modality, we
prioritized optimizing three key aspects. First, we establi…

------------------------------------------------------------------------------------------
Cluster 4 | size=6

[2501.02976] STAR: Spatial-Temporal Augmentation with Text-to-Video Models for
  Real-World Video Super-Resolution
URL: https://huggingface.co/papers/2501.02976
Summary: Image diffusion models have been adapted for real-world video
super-resolution to tackle over-smoothing issues in GAN-based methods. However,
these models struggle to maintain temporal consistency, as they are trained on
static images, limiting their ability to capture temporal dynamics effectively.
Integrating text-to-video (T2V) models into video…

[2501.00103] LTX-Video: Realtime Video Latent Diffusion
URL: https://huggingface.co/papers/2501.00103
Summary: We introduce LTX-Video, a transformer-based latent diffusion model that
adopts a holistic approach to video generation by seamlessly integrating the
responsibilities of the Video-VAE and the denoising transformer. Unlike
existing methods, which treat these components as independent, LTX-Video aims
to optimize their interaction for improved efficien…

[2501.12202] Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D
  Assets Generation
URL: https://huggingface.co/papers/2501.12202
Summary: We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for
generating high-resolution textured 3D assets. This system includes two
foundation components: a large-scale shape generation model -- Hunyuan3D-DiT,
and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape
generative model, built on a scalable flow-based diffu…

[2501.05441] The GAN is dead; long live the GAN! A Modern GAN Baseline
URL: https://huggingface.co/papers/2501.05441
Summary: There is a widely-spread claim that GANs are difficult to train, and GAN
architectures in the literature are littered with empirical tricks. We provide
evidence against this claim and build a modern GAN baseline in a more
principled manner. First, we derive a well-behaved regularized relativistic GAN
loss that addresses issues of mode dropping and …

[2501.01895] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation
URL: https://huggingface.co/papers/2501.01895
Summary: We introduce EnerVerse, a comprehensive framework for embodied future space
generation specifically designed for robotic manipulation tasks. EnerVerse
seamlessly integrates convolutional and bidirectional attention mechanisms for
inner-chunk space modeling, ensuring low-level consistency and continuity.
Recognizing the inherent redundancy in video …

[2501.08332] MangaNinja: Line Art Colorization with Precise Reference Following
URL: https://huggingface.co/papers/2501.08332
Summary: Derived from diffusion models, MangaNinjia specializes in the task of
reference-guided line art colorization. We incorporate two thoughtful designs
to ensure precise character detail transcription, including a patch shuffling
module to facilitate correspondence learning between the reference color image
and the target line art, and a point-driven c…

------------------------------------------------------------------------------------------
Cluster 1 | size=5

[2501.01257] CodeElo: Benchmarking Competition-level Code Generation of LLMs with
  Human-comparable Elo Ratings
URL: https://huggingface.co/papers/2501.01257
Summary: With the increasing code reasoning capabilities of existing large language
models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,
there is a growing need to develop more challenging and comprehensive
benchmarks that effectively test their sophisticated competition-level coding
abilities. Existing benchmarks, like LiveCodeBench a…

[2501.14249] Humanity's Last Exam
URL: https://huggingface.co/papers/2501.14249
Summary: Benchmarks are important tools for tracking the rapid advancements in large
language model (LLM) capabilities. However, benchmarks are not keeping pace in
difficulty: LLMs now achieve over 90\% accuracy on popular benchmarks like
MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In
response, we introduce Humanity's Last Exam…

[2501.04227] Agent Laboratory: Using LLM Agents as Research Assistants
URL: https://huggingface.co/papers/2501.04227
Summary: Historically, scientific discovery has been a lengthy and costly process,
demanding substantial time and resources from initial conception to final
results. To accelerate scientific discovery, reduce research costs, and improve
research quality, we introduce Agent Laboratory, an autonomous LLM-based
framework capable of completing the entire resear…

[2501.14342] Chain-of-Retrieval Augmented Generation
URL: https://huggingface.co/papers/2501.14342
Summary: This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieva…

[2412.19723] OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis
URL: https://huggingface.co/papers/2412.19723
Summary: Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or s…

------------------------------------------------------------------------------------------
Cluster 3 | size=3

[2501.05727] Enabling Scalable Oversight via Self-Evolving Critic
URL: https://huggingface.co/papers/2501.05727
Summary: Despite their remarkable performance, the development of Large Language
Models (LLMs) faces a critical challenge in scalable oversight: providing
effective feedback for tasks where human evaluation is difficult or where LLMs
outperform humans. While there is growing interest in using LLMs for critique,
current approaches still rely on human annotat…

[2501.17703] Critique Fine-Tuning: Learning to Critique is More Effective than
  Learning to Imitate
URL: https://huggingface.co/papers/2501.17703
Summary: Supervised Fine-Tuning (SFT) is commonly used to train language models to
imitate annotated responses for given instructions. In this paper, we challenge
this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models
learn to critique noisy responses rather than simply imitate correct ones.
Inspired by human learning processes that e…

[2501.18585] Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs
URL: https://huggingface.co/papers/2501.18585
Summary: Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable
abilities in complex reasoning tasks by scaling test-time compute and
exhibiting human-like deep thinking. However, we identify a phenomenon we term
underthinking, where o1-like LLMs frequently switch between different reasoning
thoughts without sufficiently exploring pro…
