{
  "source": "hf_monthly",
  "period_start": "2025-03-01",
  "period_end": "2025-03-31",
  "raw_json": "",
  "embed_config": {
    "embed_config_id": "algo_lib.embedding|0.1.0",
    "json_payload": {
      "model_name": "BAAI/bge-small-en-v1.5",
      "mode": "C",
      "top_n_keywords": 10
    }
  },
  "cluster_config": {
    "cluster_config_id": "algo_lib.clustering|0.1.0",
    "json_payload": {
      "k": 5,
      "seed": 42,
      "algorithm": "kmeans"
    }
  },
  "papers": [
    {
      "paper_id": "hf:2503.03601",
      "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
      "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
      "keywords": [
        "Sparse Autoencoders",
        "residual stream",
        "interpretable features",
        "domain-specific statistics",
        "steering approach",
        "model-specific statistics"
      ],
      "url": "https://huggingface.co/papers/2503.03601",
      "published_at": "2025-03-05"
    },
    {
      "paper_id": "hf:2503.10622",
      "title": "Transformers without Normalization",
      "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
      "keywords": [
        "Normalization layers",
        "Dynamic Tanh",
        "DyT",
        "layer normalization",
        "Transformers",
        "input-output mappings",
        "hyperparameter tuning",
        "recognition",
        "generation",
        "supervised learning",
        "self-supervised learning",
        "computer vision",
        "language models"
      ],
      "url": "https://huggingface.co/papers/2503.10622",
      "published_at": "2025-03-13"
    },
    {
      "paper_id": "hf:2503.20215",
      "title": "Qwen2.5-Omni Technical Report",
      "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
      "keywords": [
        "block-wise processing",
        "TMRoPE (Time-aligned Multimodal RoPE)",
        "Thinker-Talker architecture",
        "dual-track autoregressive model",
        "sliding-window DiT",
        "Omni-Bench",
        "MMLU",
        "GSM8K",
        "speech instruction following"
      ],
      "url": "https://huggingface.co/papers/2503.20215",
      "published_at": "2025-03-26"
    },
    {
      "paper_id": "hf:2503.14456",
      "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
      "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
      "keywords": [
        "sequence modeling architecture",
        "delta rule",
        "vector-valued gating",
        "in-context learning rates",
        "relaxed value replacement rule",
        "state tracking",
        "parallelizability",
        "Transformers",
        "$\\mathsf{TC}^0$",
        "multilingual corpus"
      ],
      "url": "https://huggingface.co/papers/2503.14456",
      "published_at": "2025-03-18"
    },
    {
      "paper_id": "hf:2503.11647",
      "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
      "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
      "keywords": [
        "camera-controlled generative video re-rendering",
        "pre-trained text-to-video models",
        "video conditioning mechanism",
        "Unreal Engine 5",
        "video stabilization",
        "super-resolution",
        "outpainting"
      ],
      "url": "https://huggingface.co/papers/2503.11647",
      "published_at": "2025-03-14"
    },
    {
      "paper_id": "hf:2503.14476",
      "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
      "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
      "keywords": [
        "LLMs",
        "reinforcement learning",
        "decoupled clip",
        "dynamic sampling policy optimization",
        "DAPO",
        "large-scale RL",
        "open-source",
        "AIME 2024",
        "Qwen2.5-32B",
        "verl framework"
      ],
      "url": "https://huggingface.co/papers/2503.14476",
      "published_at": "2025-03-18"
    },
    {
      "paper_id": "hf:2503.23307",
      "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
      "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
      "keywords": [
        "MoCha",
        "speech-video window attention mechanism",
        "joint training strategy",
        "structured prompt templates",
        "multi-character conversation",
        "cinematic coherence"
      ],
      "url": "https://huggingface.co/papers/2503.23307",
      "published_at": "2025-03-30"
    },
    {
      "paper_id": "hf:2503.06053",
      "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
      "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
      "keywords": [
        "spatio-temporal consistency",
        "video generation",
        "plot plausibility",
        "visual consistency",
        "camera movement",
        "object actions",
        "DropletVideo-10M",
        "DropletVideo model"
      ],
      "url": "https://huggingface.co/papers/2503.06053",
      "published_at": "2025-03-08"
    },
    {
      "paper_id": "hf:2502.21263",
      "title": "RuCCoD: Towards Automated ICD Coding in Russian",
      "summary": "This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.",
      "keywords": [
        "BERT",
        "LLaMA",
        "LoRA",
        "RAG",
        "transfer learning",
        "EHR",
        "ICD coding",
        "clinical coding",
        "UMLS concepts",
        "automated predicted codes"
      ],
      "url": "https://huggingface.co/papers/2502.21263",
      "published_at": "2025-02-28"
    },
    {
      "paper_id": "hf:2503.11576",
      "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
      "summary": "We introduce SmolDocling, an ultra-compact vision-language model targeting\nend-to-end document conversion. Our model comprehensively processes entire\npages by generating DocTags, a new universal markup format that captures all\npage elements in their full context with location. Unlike existing approaches\nthat rely on large foundational models, or ensemble solutions that rely on\nhandcrafted pipelines of multiple specialized models, SmolDocling offers an\nend-to-end conversion for accurately capturing content, structure and spatial\nlocation of document elements in a 256M parameters vision-language model.\nSmolDocling exhibits robust performance in correctly reproducing document\nfeatures such as code listings, tables, equations, charts, lists, and more\nacross a diverse range of document types including business documents, academic\npapers, technical reports, patents, and forms -- significantly extending beyond\nthe commonly observed focus on scientific papers. Additionally, we contribute\nnovel publicly sourced datasets for charts, tables, equations, and code\nrecognition. Experimental results demonstrate that SmolDocling competes with\nother Vision Language Models that are up to 27 times larger in size, while\nreducing computational requirements substantially. The model is currently\navailable, datasets will be publicly available soon.",
      "keywords": [
        "vision-language model",
        "DocTags",
        "end-to-end document conversion",
        "universal markup format",
        "page elements",
        "location context",
        "large foundational models",
        "ensemble solutions",
        "specialized models",
        "code listings",
        "tables",
        "equations",
        "charts",
        "lists",
        "diverse document types",
        "publicly sourced datasets",
        "Vision Language Models",
        "computational requirements"
      ],
      "url": "https://huggingface.co/papers/2503.11576",
      "published_at": "2025-03-14"
    },
    {
      "paper_id": "hf:2503.05236",
      "title": "Unified Reward Model for Multimodal Understanding and Generation",
      "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
      "keywords": [
        "UnifiedReward",
        "reward models",
        "preference optimization",
        "pairwise ranking",
        "pointwise scoring",
        "vision model preference alignment",
        "Direct Preference Optimization (DPO)"
      ],
      "url": "https://huggingface.co/papers/2503.05236",
      "published_at": "2025-03-07"
    },
    {
      "paper_id": "hf:2503.18878",
      "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
      "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
      "keywords": [
        "Sparse Autoencoders",
        "SAEs",
        "latent representations",
        "interpretable features",
        "reasoning features",
        "empirical analysis",
        "interpretability methods",
        "reasoning performance"
      ],
      "url": "https://huggingface.co/papers/2503.18878",
      "published_at": "2025-03-24"
    },
    {
      "paper_id": "hf:2503.04625",
      "title": "START: Self-taught Reasoner with Tools",
      "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
      "keywords": [
        "Large reasoning models",
        "Chain-of-thought",
        "CoT",
        "self-checking",
        "self-debugging",
        "self-learning framework",
        "Hint-infer",
        "Hint Rejection Sampling Fine-Tuning",
        "Hint-RFT",
        "QwQ-32B",
        "PhD-level science QA",
        "GPQA",
        "AMC23",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "R1-Distill-Qwen-32B",
        "o1-Preview"
      ],
      "url": "https://huggingface.co/papers/2503.04625",
      "published_at": "2025-03-06"
    },
    {
      "paper_id": "hf:2503.07920",
      "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
      "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
      "keywords": [
        "vision-language research",
        "image crawling",
        "generative vision models",
        "image generation",
        "cultural relevance"
      ],
      "url": "https://huggingface.co/papers/2503.07920",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.04130",
      "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
      "summary": "Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(Spatiotemporal TOken Reduction for\nMultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to 8times and the decoding latency by\n2.4-2.9times for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm",
      "keywords": [
        "Spatiotemporal Token Reduction",
        "temporal encoder",
        "Mamba State Space Model",
        "image tokens",
        "inter-frame dynamics",
        "token reduction",
        "test-time sampling",
        "training-based temporal and spatial pooling",
        "video reasoning",
        "video understanding",
        "MLVU",
        "LongVideoBench"
      ],
      "url": "https://huggingface.co/papers/2503.04130",
      "published_at": "2025-03-06"
    },
    {
      "paper_id": "hf:2503.16416",
      "title": "Survey on Evaluation of LLM-based Agents",
      "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
      "keywords": [
        "LLM-based agents",
        "autonomous systems",
        "evaluation methodologies",
        "planning",
        "tool use",
        "self-reflection",
        "memory",
        "web agents",
        "software engineering agents",
        "scientific agents",
        "conversational agents",
        "generalist agents",
        "evaluation frameworks",
        "realistic evaluations",
        "continuously updated benchmarks",
        "cost-efficiency",
        "safety",
        "robustness",
        "fine-grained evaluation methods",
        "scalable evaluation methods"
      ],
      "url": "https://huggingface.co/papers/2503.16416",
      "published_at": "2025-03-20"
    },
    {
      "paper_id": "hf:2503.13358",
      "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
      "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.",
      "keywords": [
        "diffusion models",
        "super-resolution",
        "SinSR",
        "OSEDiff",
        "ResShift",
        "distillation method",
        "image restoration",
        "perceptual quality",
        "degraded input images",
        "real-world datasets",
        "synthetic datasets",
        "RealSR",
        "RealSet65",
        "DRealSR",
        "ImageNet",
        "DIV2K"
      ],
      "url": "https://huggingface.co/papers/2503.13358",
      "published_at": "2025-03-17"
    },
    {
      "paper_id": "hf:2503.10633",
      "title": "Charting and Navigating Hugging Face's Model Atlas",
      "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "keywords": [
        "model landscape",
        "model evolution",
        "predicting model attributes",
        "computer vision models",
        "structural priors",
        "mapping undocumented areas",
        "interactive atlas"
      ],
      "url": "https://huggingface.co/papers/2503.10633",
      "published_at": "2025-03-13"
    },
    {
      "paper_id": "hf:2503.18942",
      "title": "Video-T1: Test-Time Scaling for Video Generation",
      "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
      "keywords": [
        "Test-Time Scaling (TTS)",
        "video generation",
        "test-time verifiers",
        "Gaussian noise space",
        "target video distribution",
        "linear search",
        "Tree-of-Frames (ToF)",
        "autoregressive",
        "text-conditioned video generation benchmarks"
      ],
      "url": "https://huggingface.co/papers/2503.18942",
      "published_at": "2025-03-24"
    },
    {
      "paper_id": "hf:2503.01743",
      "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAs",
      "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable\nlanguage and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language\nmodel trained on high-quality web and synthetic data, significantly\noutperforming recent open-source models of similar size and matching the\nperformance of models twice its size on math and coding tasks requiring complex\nreasoning. This achievement is driven by a carefully curated synthetic data\nrecipe emphasizing high-quality math and coding datasets. Compared to its\npredecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of\n200K tokens to better support multilingual applications, as well as group query\nattention for more efficient long-sequence generation. Phi-4-Multimodal is a\nmultimodal model that integrates text, vision, and speech/audio input\nmodalities into a single model. Its novel modality extension approach leverages\nLoRA adapters and modality-specific routers to allow multiple inference modes\ncombining various modalities without interference. For example, it now ranks\nfirst in the OpenASR leaderboard to date, although the LoRA component of the\nspeech/audio modality has just 460 million parameters. Phi-4-Multimodal\nsupports scenarios involving (vision + language), (vision + speech), and\n(speech/audio) inputs, outperforming larger vision-language and speech-language\nmodels on a wide range of tasks. Additionally, we experiment to further train\nPhi-4-Mini to enhance its reasoning capabilities. Despite its compact\n3.8-billion-parameter size, this experimental version achieves reasoning\nperformance on par with or surpassing significantly larger models, including\nDeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",
      "keywords": [
        "group query attention",
        "LoRA adapters",
        "modality-specific routers",
        "OpenASR",
        "DeepSeek-R1-Distill-Qwen-7B",
        "DeepSeek-R1-Distill-Llama-8B"
      ],
      "url": "https://huggingface.co/papers/2503.01743",
      "published_at": "2025-03-03"
    },
    {
      "paper_id": "hf:2503.07536",
      "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
      "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
      "keywords": [
        "Large Multimodal Models",
        "LMMs",
        "rule-based reinforcement learning",
        "RL",
        "Foundational Reasoning Enhancement",
        "FRE",
        "Multimodal Generalization Training",
        "MGT",
        "multimodal reasoning",
        "multimodal pretraining"
      ],
      "url": "https://huggingface.co/papers/2503.07536",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.07677",
      "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
      "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
      "keywords": [
        "diffusion models",
        "Classifier-Free Guidance (CFG)",
        "guidance-distilled models",
        "neural function evaluations (NFEs)",
        "PLADIS",
        "U-Net",
        "Transformer",
        "sparse attention",
        "softmax",
        "cross-attention layer",
        "text-to-image"
      ],
      "url": "https://huggingface.co/papers/2503.07677",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.01785",
      "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
      "summary": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1\nlearns from feedback on its answers, which is especially useful in applications\nwhen fine-tuning data is scarce. Recent open-source work like DeepSeek-R1\ndemonstrates that reinforcement learning with verifiable reward is one key\ndirection in reproducing o1. While the R1-style model has demonstrated success\nin language models, its application in multi-modal domains remains\nunder-explored. This work introduces Visual Reinforcement Fine-Tuning\n(Visual-RFT), which further extends the application areas of RFT on visual\ntasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs)\nto generate multiple responses containing reasoning tokens and final answers\nfor each input, and then uses our proposed visual perception verifiable reward\nfunctions to update the model via the policy optimization algorithm such as\nGroup Relative Policy Optimization (GRPO). We design different verifiable\nreward functions for different perception tasks, such as the Intersection over\nUnion (IoU) reward for object detection. Experimental results on fine-grained\nimage classification, few-shot object detection, reasoning grounding, as well\nas open-vocabulary object detection benchmarks show the competitive performance\nand advanced generalization ability of Visual-RFT compared with Supervised\nFine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over\nthe baseline in one-shot fine-grained image classification with around 100\nsamples. In few-shot object detection, Visual-RFT also exceeds the baseline by\n21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents\na paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven\napproach that enhances reasoning and adaptability for domain-specific tasks.",
      "keywords": [
        "Reinforcement Fine-Tuning (RFT)",
        "Visual Reinforcement Fine-Tuning (Visual-RFT)",
        "Large Vision-Language Models (LVLMs)",
        "reasoning tokens",
        "policy optimization algorithm",
        "Group Relative Policy Optimization (GRPO)",
        "Intersection over Union (IoU)",
        "fine-grained image classification",
        "few-shot object detection",
        "reasoning grounding",
        "open-vocabulary object detection",
        "Supervised Fine-tuning (SFT)"
      ],
      "url": "https://huggingface.co/papers/2503.01785",
      "published_at": "2025-03-03"
    },
    {
      "paper_id": "hf:2503.21460",
      "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
      "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
      "keywords": [
        "Large Language Model (LLM)",
        "goal-driven behaviors",
        "dynamic adaptation capabilities",
        "architectural foundations",
        "collaboration mechanisms",
        "evolutionary pathways",
        "agent design principles",
        "emergent behaviors",
        "evaluation methodologies",
        "tool applications",
        "practical challenges",
        "application domains"
      ],
      "url": "https://huggingface.co/papers/2503.21460",
      "published_at": "2025-03-27"
    },
    {
      "paper_id": "hf:2503.05500",
      "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
      "summary": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
      "keywords": [
        "bidirectional encoder models",
        "generative decoder-only models",
        "multilingual encoders",
        "EuroBERT",
        "multilingual capabilities",
        "token sequences"
      ],
      "url": "https://huggingface.co/papers/2503.05500",
      "published_at": "2025-03-07"
    },
    {
      "paper_id": "hf:2503.21776",
      "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
      "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
      "keywords": [
        "rule-based reinforcement learning",
        "RL",
        "GRPO algorithm",
        "T-GRPO algorithm",
        "temporal modeling",
        "multimodal large language models",
        "MLLMs",
        "SFT cold start",
        "video reasoning benchmarks",
        "VideoMMMU",
        "VSI-Bench",
        "MVBench",
        "TempCompass",
        "video spatial reasoning"
      ],
      "url": "https://huggingface.co/papers/2503.21776",
      "published_at": "2025-03-27"
    },
    {
      "paper_id": "hf:2503.10613",
      "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
      "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
      "keywords": [
        "stable diffusion",
        "DALLE-3",
        "multi-turn image editing",
        "agentic workflow",
        "tool use",
        "subtasks",
        "large language models (LLMs)",
        "graph search",
        "A* search",
        "subtask tree",
        "subgraph",
        "vision-language model (VLM)",
        "cost-efficient tool paths",
        "cost-quality trade-off",
        "multi-turn image editing benchmark"
      ],
      "url": "https://huggingface.co/papers/2503.10613",
      "published_at": "2025-03-13"
    },
    {
      "paper_id": "hf:2503.16419",
      "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Large Reasoning Models (LRMs)",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "Chain-of-Thought (CoT) reasoning",
        "overthinking phenomenon",
        "efficient reasoning",
        "model-based efficient reasoning",
        "reasoning output-based efficient reasoning",
        "input prompts-based efficient reasoning",
        "efficient data",
        "small language models",
        "evaluation methods",
        "benchmarking"
      ],
      "url": "https://huggingface.co/papers/2503.16419",
      "published_at": "2025-03-20"
    },
    {
      "paper_id": "hf:2503.19693",
      "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
      "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
      "keywords": [
        "Large Language Models",
        "LLMs",
        "domain adaptation",
        "auto-regressive decoding",
        "vocabulary adaptation",
        "AdaptiVocab",
        "n-gram-based tokens",
        "token embeddings",
        "fine-tuning",
        "end-to-end approach"
      ],
      "url": "https://huggingface.co/papers/2503.19693",
      "published_at": "2025-03-25"
    },
    {
      "paper_id": "hf:2503.09573",
      "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
      "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
      "keywords": [
        "diffusion language models",
        "autoregressive models",
        "denoising diffusion",
        "block diffusion",
        "parallelized generation",
        "controllability",
        "likelihood modeling",
        "fixed-length generation",
        "flexible-length generation",
        "inference efficiency",
        "KV caching",
        "parallel token sampling",
        "efficient training algorithm",
        "gradient variance estimators",
        "data-driven noise schedules",
        "language modeling benchmarks",
        "arbitrary-length sequences"
      ],
      "url": "https://huggingface.co/papers/2503.09573",
      "published_at": "2025-03-12"
    },
    {
      "paper_id": "hf:2503.19325",
      "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
      "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
      "keywords": [
        "Frame AutoRegressive (FAR)",
        "video autoregressive modeling",
        "causal dependencies",
        "Token AR",
        "video diffusion transformers",
        "long-context vision modeling",
        "visual redundancy",
        "RoPE",
        "temporal decay",
        "long video sequences",
        "FlexRoPE",
        "long short-term context modeling",
        "fine-grained temporal consistency",
        "long-range information",
        "short-video generation",
        "long-video generation"
      ],
      "url": "https://huggingface.co/papers/2503.19325",
      "published_at": "2025-03-25"
    },
    {
      "paper_id": "hf:2503.16660",
      "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
      "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
      "keywords": [
        "autoencoder",
        "Gumbel-Softmax selection mechanism",
        "visual tokens",
        "feature utility",
        "informative visual tokens",
        "LLaVA-NeXT",
        "OCR-based tasks",
        "general-domain tasks",
        "multimodal pruning",
        "scalable inference",
        "low-overhead inference"
      ],
      "url": "https://huggingface.co/papers/2503.16660",
      "published_at": "2025-03-20"
    },
    {
      "paper_id": "hf:2503.04724",
      "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
      "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
      "keywords": [
        "LLM",
        "multimodal interactions",
        "fine-tuning",
        "computational overhead",
        "text-speech misalignment",
        "autoregressive streaming TTS",
        "Word Error Rate",
        "latency",
        "UTMOS score",
        "multi-queue token streaming",
        "infinite-length dialogues",
        "plug-and-play",
        "dataset adaptation",
        "Character Error Rate",
        "Vision-Language Model",
        "omni-model"
      ],
      "url": "https://huggingface.co/papers/2503.04724",
      "published_at": "2025-03-06"
    },
    {
      "paper_id": "hf:2503.08638",
      "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
      "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "keywords": [
        "track-decoupled next-token prediction",
        "structural progressive conditioning",
        "multitask",
        "multiphase pre-training",
        "in-context learning",
        "style transfer",
        "bidirectional generation",
        "music generation",
        "music understanding",
        "MARBLE benchmark"
      ],
      "url": "https://huggingface.co/papers/2503.08638",
      "published_at": "2025-03-11"
    },
    {
      "paper_id": "hf:2503.12533",
      "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
      "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
      "keywords": [
        "Foundation Models",
        "hierarchical agent framework",
        "modular skill library",
        "lightweight vision-language model",
        "vision-language model",
        "Connector module"
      ],
      "url": "https://huggingface.co/papers/2503.12533",
      "published_at": "2025-03-16"
    },
    {
      "paper_id": "hf:2503.07605",
      "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
      "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
      "keywords": [
        "Sparse Expert Activation Pruning",
        "SEAP",
        "large language models",
        "computational overhead",
        "task-specific expert activation patterns"
      ],
      "url": "https://huggingface.co/papers/2503.07605",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.00865",
      "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of\n  Global Speakers",
      "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce Babel, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: Babel-9B, designed for efficient inference and\nfine-tuning, and Babel-83B, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.",
      "keywords": [
        "LLMs",
        "natural language processing",
        "multilingual LLMs",
        "layer extension",
        "Babel-9B",
        "Babel-83B",
        "inference",
        "fine-tuning",
        "supervised fine-tuning",
        "multilingual tasks"
      ],
      "url": "https://huggingface.co/papers/2503.00865",
      "published_at": "2025-03-02"
    },
    {
      "paper_id": "hf:2503.21620",
      "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
      "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
      "keywords": [
        "reinforcement learning",
        "multimodal large language models",
        "GUI action prediction",
        "Group Relative Policy Optimization",
        "policy-based algorithms",
        "AndroidControl",
        "ScreenSpot-Pro",
        "supervised fine-tuning"
      ],
      "url": "https://huggingface.co/papers/2503.21620",
      "published_at": "2025-03-27"
    },
    {
      "paper_id": "hf:2503.14378",
      "title": "Impossible Videos",
      "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
      "keywords": [
        "video generation models",
        "prompt following",
        "creativity",
        "video understanding models",
        "temporal dynamics",
        "world knowledge",
        "video-LLMs",
        "IPV-Bench"
      ],
      "url": "https://huggingface.co/papers/2503.14378",
      "published_at": "2025-03-18"
    },
    {
      "paper_id": "hf:2503.07365",
      "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
      "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
      "keywords": [
        "rule-based reinforcement learning",
        "multimodal reasoning",
        "DeepSeek-R1",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "data efficiency"
      ],
      "url": "https://huggingface.co/papers/2503.07365",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.05132",
      "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
      "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
      "keywords": [
        "reinforcement learning",
        "large language models",
        "multimodal reasoning",
        "Qwen2-VL-2B",
        "SAT dataset",
        "CVBench",
        "instruct models",
        "trivial reasoning trajectories",
        "naive length reward"
      ],
      "url": "https://huggingface.co/papers/2503.05132",
      "published_at": "2025-03-07"
    },
    {
      "paper_id": "hf:2503.20314",
      "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
      "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
      "keywords": [
        "diffusion transformer",
        "VAE",
        "scalable pre-training",
        "large-scale data curation",
        "video generation",
        "image-to-video",
        "instruction-guided video editing",
        "personal video generation",
        "VRAM"
      ],
      "url": "https://huggingface.co/papers/2503.20314",
      "published_at": "2025-03-26"
    },
    {
      "paper_id": "hf:2503.15299",
      "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
      "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
      "keywords": [
        "large language models",
        "LLMs",
        "knowledge",
        "token-level probabilities",
        "intermediate computations",
        "external knowledge",
        "internal knowledge",
        "hidden knowledge",
        "closed-book QA",
        "generation capabilities"
      ],
      "url": "https://huggingface.co/papers/2503.15299",
      "published_at": "2025-03-19"
    },
    {
      "paper_id": "hf:2503.07598",
      "title": "VACE: All-in-One Video Creation and Editing",
      "summary": "Diffusion Transformer has demonstrated powerful capability and scalability in\ngenerating high-quality images and videos. Further pursuing the unification of\ngeneration and editing tasks has yielded significant progress in the domain of\nimage content creation. However, due to the intrinsic demands for consistency\nacross both temporal and spatial dynamics, achieving a unified approach for\nvideo synthesis remains challenging. We introduce VACE, which enables users to\nperform Video tasks within an All-in-one framework for Creation and Editing.\nThese tasks include reference-to-video generation, video-to-video editing, and\nmasked video-to-video editing. Specifically, we effectively integrate the\nrequirements of various tasks by organizing video task inputs, such as editing,\nreference, and masking, into a unified interface referred to as the Video\nCondition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we\ninject different task concepts into the model using formalized representations\nof temporal and spatial dimensions, allowing it to handle arbitrary video\nsynthesis tasks flexibly. Extensive experiments demonstrate that the unified\nmodel of VACE achieves performance on par with task-specific models across\nvarious subtasks. Simultaneously, it enables diverse applications through\nversatile task combinations. Project page:\nhttps://ali-vilab.github.io/VACE-Page/.",
      "keywords": [
        "diffusion transformer",
        "video synthesis",
        "reference-to-video generation",
        "video-to-video editing",
        "masked video-to-video editing",
        "video condition unit",
        "context adapter"
      ],
      "url": "https://huggingface.co/papers/2503.07598",
      "published_at": "2025-03-10"
    },
    {
      "paper_id": "hf:2503.00808",
      "title": "Predictive Data Selection: The Data That Predicts Is the Data That\n  Teaches",
      "summary": "Language model pretraining involves training on extensive corpora, where data\nquality plays a pivotal role. In this work, we aim to directly estimate the\ncontribution of data during pretraining and select pretraining data in an\nefficient manner. Specifically, we draw inspiration from recent findings\nshowing that compression efficiency (i.e., the normalized loss) of diverse\nmodels on certain text correlates strongly with their downstream performance,\nwhen the text domain aligns with the downstream benchmark (Huang et al., 2024).\nBuilding on this observation, we hypothesize that data on which model losses\nare predictive of downstream abilities also contribute effectively to learning.\nTo leverage this insight, we introduce data selection based on data's\nPredictive strength (Preselect), a lightweight and efficient data selection\nmethod that requires training and deploying only a fastText-based scorer.\nThrough comprehensive experiments with 1B and 3B parameter models, we\ndemonstrate that models trained on 30B tokens selected with PreSelect surpasses\nthe performance of a vanilla baseline trained on 300B tokens, achieving a 10x\nreduction in compute requirements. Furthermore, PreSelect significantly\noutperforms other competitive data selection baselines, such as DCLM and\nFineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our\ntrained data selection scorer along with the curated datasets at\nhttps://github.com/hkust-nlp/PreSelect.",
      "keywords": [
        "compression efficiency",
        "downstream performance",
        "data selection",
        "fastText",
        "predictive strength",
        "DCLM",
        "FineWeb-Edu"
      ],
      "url": "https://huggingface.co/papers/2503.00808",
      "published_at": "2025-03-02"
    },
    {
      "paper_id": "hf:2503.10480",
      "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
      "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
      "keywords": [
        "large vision-language models",
        "embodied task planning",
        "dependency constraints",
        "efficiency",
        "world models",
        "preference learning",
        "Dual Preference Optimization",
        "D$^2$PO",
        "state prediction",
        "action selection",
        "tree search",
        "VoTa-Bench",
        "Qwen2-VL",
        "LLaVA-1.6",
        "LLaMA-3.2",
        "task success rates",
        "execution paths"
      ],
      "url": "https://huggingface.co/papers/2503.10480",
      "published_at": "2025-03-13"
    },
    {
      "paper_id": "hf:2503.16905",
      "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
      "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
      "keywords": [
        "Multi-Agent framework",
        "Big Seven Personality",
        "Socratic guidance",
        "MSR",
        "EMMA",
        "Olympiad",
        "MathVista",
        "Critic agent"
      ],
      "url": "https://huggingface.co/papers/2503.16905",
      "published_at": "2025-03-21"
    },
    {
      "paper_id": "hf:2503.19786",
      "title": "Gemma 3 Technical Report",
      "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
      "keywords": [
        "multimodal models",
        "vision understanding",
        "long context",
        "KV-cache memory",
        "local attention",
        "global attention",
        "model distillation",
        "pre-trained models",
        "instruction finetuned",
        "post-training recipe",
        "math ability",
        "chat ability",
        "instruction-following",
        "multilingual abilities"
      ],
      "url": "https://huggingface.co/papers/2503.19786",
      "published_at": "2025-03-25"
    },
    {
      "paper_id": "hf:2503.10639",
      "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
      "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
      "keywords": [
        "Generation Chain-of-Thought",
        "GoT",
        "diffusion model",
        "Semantic-Spatial Guidance Module",
        "Qwen2.5-VL",
        "reasoning chain",
        "text-to-image generation",
        "interactive visual generation"
      ],
      "url": "https://huggingface.co/papers/2503.10639",
      "published_at": "2025-03-13"
    },
    {
      "paper_id": "hf:2503.16219",
      "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
      "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
      "keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "GRPO",
        "mathematical reasoning",
        "AMC23",
        "AIME24",
        "parameter-efficient fine-tuning",
        "optimization instability",
        "length constraints",
        "scalable LLMs"
      ],
      "url": "https://huggingface.co/papers/2503.16219",
      "published_at": "2025-03-20"
    }
  ],
  "clusters": [
    {
      "cluster_index": 0,
      "size": 4,
      "cohesion": 0.9266725182533264,
      "members": [
        {
          "paper_id": "hf:2503.20314",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9470505714416504
        },
        {
          "paper_id": "hf:2503.18942",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9310946464538574
        },
        {
          "paper_id": "hf:2503.07598",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9194920063018799
        },
        {
          "paper_id": "hf:2503.14378",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.909052848815918
        }
      ]
    },
    {
      "cluster_index": 1,
      "size": 17,
      "cohesion": 0.8948555834153119,
      "members": [
        {
          "paper_id": "hf:2503.16419",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9261811971664429
        },
        {
          "paper_id": "hf:2503.18878",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9214396476745605
        },
        {
          "paper_id": "hf:2503.16219",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9168075323104858
        },
        {
          "paper_id": "hf:2503.00865",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.9146198034286499
        },
        {
          "paper_id": "hf:2503.19693",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.9051814675331116
        },
        {
          "paper_id": "hf:2503.15299",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.9048120975494385
        },
        {
          "paper_id": "hf:2503.14456",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.902245044708252
        },
        {
          "paper_id": "hf:2503.01743",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.901653528213501
        },
        {
          "paper_id": "hf:2503.21460",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.8982335329055786
        },
        {
          "paper_id": "hf:2503.07605",
          "rank_in_cluster": 9,
          "sim_to_centroid": 0.894341766834259
        },
        {
          "paper_id": "hf:2503.14476",
          "rank_in_cluster": 10,
          "sim_to_centroid": 0.8923249244689941
        },
        {
          "paper_id": "hf:2503.05500",
          "rank_in_cluster": 11,
          "sim_to_centroid": 0.891110897064209
        },
        {
          "paper_id": "hf:2503.04625",
          "rank_in_cluster": 12,
          "sim_to_centroid": 0.8903292417526245
        },
        {
          "paper_id": "hf:2503.00808",
          "rank_in_cluster": 13,
          "sim_to_centroid": 0.8856947422027588
        },
        {
          "paper_id": "hf:2503.03601",
          "rank_in_cluster": 14,
          "sim_to_centroid": 0.8752884864807129
        },
        {
          "paper_id": "hf:2503.16416",
          "rank_in_cluster": 15,
          "sim_to_centroid": 0.8648366928100586
        },
        {
          "paper_id": "hf:2502.21263",
          "rank_in_cluster": 16,
          "sim_to_centroid": 0.827444314956665
        }
      ]
    },
    {
      "cluster_index": 2,
      "size": 5,
      "cohesion": 0.8983314156532287,
      "members": [
        {
          "paper_id": "hf:2503.20215",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.931185245513916
        },
        {
          "paper_id": "hf:2503.04724",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9106315970420837
        },
        {
          "paper_id": "hf:2503.19786",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.892090916633606
        },
        {
          "paper_id": "hf:2503.08638",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.8900554180145264
        },
        {
          "paper_id": "hf:2503.10622",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.8676939010620117
        }
      ]
    },
    {
      "cluster_index": 3,
      "size": 9,
      "cohesion": 0.8973677555720011,
      "members": [
        {
          "paper_id": "hf:2503.19325",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9316053986549377
        },
        {
          "paper_id": "hf:2503.04130",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9117603302001953
        },
        {
          "paper_id": "hf:2503.06053",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.8997751474380493
        },
        {
          "paper_id": "hf:2503.11647",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.8997529745101929
        },
        {
          "paper_id": "hf:2503.13358",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.8970010280609131
        },
        {
          "paper_id": "hf:2503.16660",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.8931676149368286
        },
        {
          "paper_id": "hf:2503.09573",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.8861568570137024
        },
        {
          "paper_id": "hf:2503.07677",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.8842166662216187
        },
        {
          "paper_id": "hf:2503.10633",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.8728737831115723
        }
      ]
    },
    {
      "cluster_index": 4,
      "size": 15,
      "cohesion": 0.895480477809906,
      "members": [
        {
          "paper_id": "hf:2503.07365",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9266918897628784
        },
        {
          "paper_id": "hf:2503.05132",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9251584410667419
        },
        {
          "paper_id": "hf:2503.21776",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9191349148750305
        },
        {
          "paper_id": "hf:2503.10639",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.9125280380249023
        },
        {
          "paper_id": "hf:2503.07536",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.9107396006584167
        },
        {
          "paper_id": "hf:2503.21620",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.9088646173477173
        },
        {
          "paper_id": "hf:2503.10480",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.9012783169746399
        },
        {
          "paper_id": "hf:2503.01785",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.9007416367530823
        },
        {
          "paper_id": "hf:2503.05236",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.8958710432052612
        },
        {
          "paper_id": "hf:2503.10613",
          "rank_in_cluster": 9,
          "sim_to_centroid": 0.8903200626373291
        },
        {
          "paper_id": "hf:2503.07920",
          "rank_in_cluster": 10,
          "sim_to_centroid": 0.8817883133888245
        },
        {
          "paper_id": "hf:2503.12533",
          "rank_in_cluster": 11,
          "sim_to_centroid": 0.8735809326171875
        },
        {
          "paper_id": "hf:2503.23307",
          "rank_in_cluster": 12,
          "sim_to_centroid": 0.8694603443145752
        },
        {
          "paper_id": "hf:2503.16905",
          "rank_in_cluster": 13,
          "sim_to_centroid": 0.8633122444152832
        },
        {
          "paper_id": "hf:2503.11576",
          "rank_in_cluster": 14,
          "sim_to_centroid": 0.85273677110672
        }
      ]
    }
  ]
}