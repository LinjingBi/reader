{
  "source": "hf_monthly",
  "period_start": "2025-05-01",
  "period_end": "2025-05-31",
  "raw_json": "",
  "embed_config": {
    "embed_config_id": "algo_lib.embedding|0.1.0",
    "json_payload": {
      "model_name": "BAAI/bge-small-en-v1.5",
      "mode": "B",
      "top_n_keywords": 10
    }
  },
  "cluster_config": {
    "cluster_config_id": "algo_lib.clustering|0.1.0",
    "json_payload": {
      "k": 5,
      "seed": 42,
      "algorithm": "kmeans"
    }
  },
  "papers": [
    {
      "paper_id": "hf:2505.09388",
      "title": "Qwen3 Technical Report",
      "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
      "keywords": [
        "large language models",
        "dense architecture",
        "Mixture-of-Expert",
        "thinking mode",
        "non-thinking mode",
        "thinking budget mechanism",
        "knowledge transfer",
        "multilingual capabilities",
        "code generation",
        "mathematical reasoning",
        "agent tasks",
        "cross-lingual understanding",
        "generation capabilities"
      ],
      "url": "https://huggingface.co/papers/2505.09388",
      "published_at": "2025-05-14"
    },
    {
      "paper_id": "hf:2505.17894",
      "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model",
      "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.",
      "keywords": [
        "language model",
        "bidirectional Arabic-English translation",
        "LLMs",
        "Kuwain-1.5B",
        "two-phase training",
        "high-quality training corpus",
        "Tarjama-25",
        "domain narrowness",
        "English-source bias",
        "GPT-4"
      ],
      "url": "https://huggingface.co/papers/2505.17894",
      "published_at": "2025-05-23"
    },
    {
      "paper_id": "hf:2505.03335",
      "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
      "keywords": [
        "reinforcement learning with verifiable rewards",
        "zero setting",
        "self-evolving training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended learning",
        "grounded learning"
      ],
      "url": "https://huggingface.co/papers/2505.03335",
      "published_at": "2025-05-06"
    },
    {
      "paper_id": "hf:2505.04921",
      "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
      "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
      "keywords": [
        "Multimodal Reasoning Models",
        "LMRMs",
        "multimodal LLMs",
        "Multimodal Chain-of-Thought",
        "multimodal reinforcement learning",
        "OpenAI O3",
        "OpenAI O4-mini",
        "native large multimodal reasoning models",
        "N-LMRMs"
      ],
      "url": "https://huggingface.co/papers/2505.04921",
      "published_at": "2025-05-08"
    },
    {
      "paper_id": "hf:2505.07062",
      "title": "Seed1.5-VL Technical Report",
      "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
      "keywords": [
        "vision-language foundation model",
        "vision encoder",
        "Mixture-of-Experts (MoE)",
        "LLM",
        "multimodal understanding",
        "multimodal reasoning",
        "visual puzzles",
        "GUI control",
        "gameplay",
        "VLM benchmarks",
        "visual and video understanding"
      ],
      "url": "https://huggingface.co/papers/2505.07062",
      "published_at": "2025-05-11"
    },
    {
      "paper_id": "hf:2505.19147",
      "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
      "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
      "keywords": [
        "large language models",
        "multi-modal LLMs",
        "self-attention",
        "token compression",
        "long-context AI",
        "mathematical framework",
        "model efficiency",
        "long-context overhead",
        "current challenges",
        "future directions"
      ],
      "url": "https://huggingface.co/papers/2505.19147",
      "published_at": "2025-05-25"
    },
    {
      "paper_id": "hf:2505.07916",
      "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
      "summary": "We introduce MiniMax-Speech, an autoregressive Transformer-based\nText-to-Speech (TTS) model that generates high-quality speech. A key innovation\nis our learnable speaker encoder, which extracts timbre features from a\nreference audio without requiring its transcription. This enables\nMiniMax-Speech to produce highly expressive speech with timbre consistent with\nthe reference in a zero-shot manner, while also supporting one-shot voice\ncloning with exceptionally high similarity to the reference voice. In addition,\nthe overall quality of the synthesized audio is enhanced through the proposed\nFlow-VAE. Our model supports 32 languages and demonstrates excellent\nperformance across multiple objective and subjective evaluations metrics.\nNotably, it achieves state-of-the-art (SOTA) results on objective voice cloning\nmetrics (Word Error Rate and Speaker Similarity) and has secured the top\nposition on the public TTS Arena leaderboard. Another key strength of\nMiniMax-Speech, granted by the robust and disentangled representations from the\nspeaker encoder, is its extensibility without modifying the base model,\nenabling various applications such as: arbitrary voice emotion control via\nLoRA; text to voice (T2V) by synthesizing timbre features directly from text\ndescription; and professional voice cloning (PVC) by fine-tuning timbre\nfeatures with additional data. We encourage readers to visit\nhttps://minimax-ai.github.io/tts_tech_report for more examples.",
      "keywords": [
        "autoregressive Transformer",
        "Text-to-Speech",
        "TTS",
        "learnable speaker encoder",
        "timbre features",
        "zero-shot",
        "one-shot voice cloning",
        "Flow-VAE",
        "disentangled representations",
        "LoRA",
        "text to voice",
        "T2V",
        "professional voice cloning",
        "PVC"
      ],
      "url": "https://huggingface.co/papers/2505.07916",
      "published_at": "2025-05-12"
    },
    {
      "paper_id": "hf:2505.14683",
      "title": "Emerging Properties in Unified Multimodal Pretraining",
      "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
      "keywords": [
        "multimodal understanding",
        "multimodal generation",
        "foundational model",
        "decoder-only model",
        "trillions of tokens",
        "large-scale interleaved data",
        "complex multimodal reasoning",
        "free-form image manipulation",
        "future frame prediction",
        "3D manipulation",
        "world navigation"
      ],
      "url": "https://huggingface.co/papers/2505.14683",
      "published_at": "2025-05-20"
    },
    {
      "paper_id": "hf:2505.22617",
      "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
      "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
      "keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ],
      "url": "https://huggingface.co/papers/2505.22617",
      "published_at": "2025-05-28"
    },
    {
      "paper_id": "hf:2505.11820",
      "title": "Chain-of-Model Learning for Language Model",
      "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
      "keywords": [
        "Chain-of-Model (CoM)",
        "Chain-of-Representation (CoR)",
        "sub-representations",
        "input representations",
        "output representations",
        "Chain-of-Language-Model (CoLM)",
        "KV sharing mechanism",
        "prefilling acceleration",
        "Transformer architecture",
        "elastic inference",
        "progressive scaling"
      ],
      "url": "https://huggingface.co/papers/2505.11820",
      "published_at": "2025-05-17"
    },
    {
      "paper_id": "hf:2505.16938",
      "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
      "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
      "keywords": [],
      "url": "https://huggingface.co/papers/2505.16938",
      "published_at": "2025-05-22"
    },
    {
      "paper_id": "hf:2505.10554",
      "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
      "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "keywords": [
        "large reasoning models",
        "long chain-of-thought reasoning",
        "outcome-based reinforcement learning",
        "self-correction",
        "backtracking",
        "verification",
        "meta-abilities",
        "automatic task generation",
        "parameter-space merging",
        "domain-specific reinforcement learning"
      ],
      "url": "https://huggingface.co/papers/2505.10554",
      "published_at": "2025-05-15"
    },
    {
      "paper_id": "hf:2505.18125",
      "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
      "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
      "keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ],
      "url": "https://huggingface.co/papers/2505.18125",
      "published_at": "2025-05-23"
    },
    {
      "paper_id": "hf:2505.21497",
      "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers",
      "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.",
      "keywords": [
        "top-down pipeline",
        "multi-agent pipeline",
        "VLM-as-judge",
        "binary-tree layout",
        "rendering code",
        "VLM feedback",
        "parser",
        "planner",
        "painter-commenter loop",
        "GPT-4",
        "Qwen-2.5",
        "automated poster-generation models"
      ],
      "url": "https://huggingface.co/papers/2505.21497",
      "published_at": "2025-05-27"
    },
    {
      "paper_id": "hf:2505.15277",
      "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
      "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
      "keywords": [
        "multimodal large language model",
        "process reward model",
        "web navigation",
        "webPRM collection",
        "webrewardbench",
        "long-horizon sequential decision making",
        "preference pairs",
        "annotated checklists",
        "step-level assessment",
        "webarena-lite",
        "policy",
        "verifier"
      ],
      "url": "https://huggingface.co/papers/2505.15277",
      "published_at": "2025-05-21"
    },
    {
      "paper_id": "hf:2505.09568",
      "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
      "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
      "keywords": [
        "diffusion transformer",
        "CLIP image features",
        "VAE-based representations",
        "sequential pretraining",
        "image understanding",
        "image generation",
        "BLIP3-o",
        "instruction-tuning dataset"
      ],
      "url": "https://huggingface.co/papers/2505.09568",
      "published_at": "2025-05-14"
    },
    {
      "paper_id": "hf:2505.15809",
      "title": "MMaDA: Multimodal Large Diffusion Language Models",
      "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
      "keywords": [
        "multimodal diffusion foundation models",
        "unified diffusion architecture",
        "modality-agnostic design",
        "mixed long chain-of-thought fine-tuning",
        "cold-start training",
        "reinforcement learning",
        "UniGRPO",
        "policy-gradient-based RL algorithm",
        "diversified reward modeling",
        "generalization capabilities",
        "textual reasoning",
        "multimodal understanding",
        "text-to-image generation"
      ],
      "url": "https://huggingface.co/papers/2505.15809",
      "published_at": "2025-05-21"
    },
    {
      "paper_id": "hf:2505.23621",
      "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
      "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
      "keywords": [
        "distillation",
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "reasoning traces",
        "DeepSeek-R1",
        "LLMs",
        "Table-R1-SFT",
        "GRPO",
        "Table-R1-Zero",
        "short-form QA",
        "fact verification",
        "free-form QA",
        "instruction tuning",
        "model architecture choices",
        "cross-task generalization",
        "table reasoning skills"
      ],
      "url": "https://huggingface.co/papers/2505.23621",
      "published_at": "2025-05-29"
    },
    {
      "paper_id": "hf:2505.20411",
      "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
      "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
      "keywords": [
        "LLM-based agents",
        "reinforcement learning",
        "software engineering tasks",
        "GitHub repositories",
        "SWE-rebench",
        "contamination-free benchmark",
        "SWE-bench Verified"
      ],
      "url": "https://huggingface.co/papers/2505.20411",
      "published_at": "2025-05-26"
    },
    {
      "paper_id": "hf:2505.03318",
      "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
      "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
      "keywords": [
        "Reward Models",
        "multimodal Reward Models",
        "long chains of thought",
        "reasoning process",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "GPT-4o",
        "cold start",
        "large-scale unified multimodal preference data",
        "Group Relative Policy Optimization",
        "GRPO",
        "vision reward tasks"
      ],
      "url": "https://huggingface.co/papers/2505.03318",
      "published_at": "2025-05-06"
    },
    {
      "paper_id": "hf:2504.20752",
      "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
      "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
      "keywords": [
        "Transformers",
        "NLP",
        "factual reasoning",
        "grokking",
        "knowledge graphs",
        "synthetic data",
        "multi-hop reasoning",
        "2WikiMultiHopQA",
        "generalizing circuits",
        "reasoning capabilities"
      ],
      "url": "https://huggingface.co/papers/2504.20752",
      "published_at": "2025-04-29"
    },
    {
      "paper_id": "hf:2505.17667",
      "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
      "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
      "keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ],
      "url": "https://huggingface.co/papers/2505.17667",
      "published_at": "2025-05-23"
    },
    {
      "paper_id": "hf:2505.05470",
      "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
      "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
      "keywords": [
        "Flow-GRPO",
        "online reinforcement learning",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Stochastic Differential Equation",
        "Denoising Reduction",
        "text-to-image tasks",
        "RL-tuned SD3.5",
        "GenEval",
        "visual text rendering",
        "human preference alignment"
      ],
      "url": "https://huggingface.co/papers/2505.05470",
      "published_at": "2025-05-08"
    },
    {
      "paper_id": "hf:2505.02707",
      "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
      "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
      "keywords": [
        "end-to-end architecture",
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "acoustic modeling",
        "voice generation",
        "persona-aware",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation"
      ],
      "url": "https://huggingface.co/papers/2505.02707",
      "published_at": "2025-05-05"
    },
    {
      "paper_id": "hf:2505.19297",
      "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold",
      "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public.",
      "keywords": [
        "text-to-image",
        "fine-tuning",
        "pre-trained generative model",
        "general-purpose datasets",
        "aesthetic quality",
        "alignment",
        "curated datasets"
      ],
      "url": "https://huggingface.co/papers/2505.19297",
      "published_at": "2025-05-25"
    },
    {
      "paper_id": "hf:2505.21327",
      "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
      "summary": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.",
      "keywords": [
        "multimodal large language models",
        "MME-Reasoning",
        "logical reasoning",
        "inductive reasoning",
        "deductive reasoning",
        "abductive reasoning",
        "reasoning ability",
        "thinking mode",
        "Rule-based RL"
      ],
      "url": "https://huggingface.co/papers/2505.21327",
      "published_at": "2025-05-27"
    },
    {
      "paper_id": "hf:2505.13417",
      "title": "AdaptThink: Reasoning Models Can Learn When to Think",
      "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.",
      "keywords": [
        "NoThinking",
        "Reinforcement Learning (RL)",
        "constrained optimization objective",
        "importance sampling strategy",
        "on-policy training",
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "reasoning quality",
        "efficiency"
      ],
      "url": "https://huggingface.co/papers/2505.13417",
      "published_at": "2025-05-19"
    },
    {
      "paper_id": "hf:2505.10475",
      "title": "Parallel Scaling Law for Language Models",
      "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply P diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the P outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with P parallel\nstreams is similar to scaling the parameters by O(log P) while showing\nsuperior inference efficiency. For example, ParScale can use up to 22times\nless memory increase and 6times less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.",
      "keywords": [
        "parallel computation",
        "parallel scaling",
        "ParScale",
        "parameter scaling",
        "inference-time scaling",
        "scaling law",
        "inference efficiency",
        "memory increase",
        "latency increase",
        "post-training"
      ],
      "url": "https://huggingface.co/papers/2505.10475",
      "published_at": "2025-05-15"
    },
    {
      "paper_id": "hf:2505.07608",
      "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
      "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.",
      "keywords": [
        "large language model",
        "pre-training",
        "data preprocessing",
        "three-stage data mixing strategy",
        "Multi-Token Prediction",
        "post-training",
        "reinforcement learning",
        "test-difficulty-driven code-reward scheme",
        "strategic data resampling",
        "extensive evaluations",
        "mathematics",
        "programming problems",
        "general reasoning tasks"
      ],
      "url": "https://huggingface.co/papers/2505.07608",
      "published_at": "2025-05-12"
    },
    {
      "paper_id": "hf:2505.04620",
      "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
      "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
      "keywords": [
        "Multimodal Large Language Model",
        "Multimodal Generalist paradigm",
        "coarse-grained multimodal understanding",
        "fine-grained multimodal understanding",
        "General-Level",
        "evaluation framework",
        "Synergy",
        "General-Bench",
        "next-generation multimodal foundation models"
      ],
      "url": "https://huggingface.co/papers/2505.04620",
      "published_at": "2025-05-07"
    },
    {
      "paper_id": "hf:2505.17612",
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
      "keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ],
      "url": "https://huggingface.co/papers/2505.17612",
      "published_at": "2025-05-23"
    },
    {
      "paper_id": "hf:2505.02567",
      "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
      "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "keywords": [
        "multimodal understanding",
        "diffusion-based models",
        "autoregressive-based models",
        "hybrid approaches",
        "cross-modal attention"
      ],
      "url": "https://huggingface.co/papers/2505.02567",
      "published_at": "2025-05-05"
    },
    {
      "paper_id": "hf:2505.02387",
      "title": "RM-R1: Reward Modeling as Reasoning",
      "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
      "keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "high-quality reasoning chains",
        "reinforcement learning with verifiable rewards",
        "LLM rollouts",
        "reasoning traces",
        "chat-specific rubrics",
        "state-of-the-art performance",
        "reward model benchmarks",
        "Llama3.1-405B",
        "GPT-4o"
      ],
      "url": "https://huggingface.co/papers/2505.02387",
      "published_at": "2025-05-05"
    },
    {
      "paper_id": "hf:2505.14669",
      "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
      "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
      "keywords": [
        "large language models",
        "low-precision arithmetic",
        "Blackwell architecture",
        "FP4",
        "mixed-precision",
        "linear layers",
        "low-precision scaling law",
        "CUDA kernels"
      ],
      "url": "https://huggingface.co/papers/2505.14669",
      "published_at": "2025-05-20"
    },
    {
      "paper_id": "hf:2505.14302",
      "title": "Scaling Law for Quantization-Aware Training",
      "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
      "keywords": [
        "quantization-aware training",
        "QAT",
        "quantization error",
        "model size",
        "training tokens",
        "quantization granularity",
        "weight quantization",
        "activation quantization",
        "mixed-precision quantization",
        "FC2 layer"
      ],
      "url": "https://huggingface.co/papers/2505.14302",
      "published_at": "2025-05-20"
    },
    {
      "paper_id": "hf:2505.11594",
      "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
      "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
      "keywords": [
        "attention",
        "FP4 Tensor Cores",
        "Blackwell GPUs",
        "RTX5090",
        "TOPS",
        "FlashAttention",
        "low-bit attention",
        "forward propagation",
        "backward propagation",
        "fine-tuning",
        "pretraining"
      ],
      "url": "https://huggingface.co/papers/2505.11594",
      "published_at": "2025-05-16"
    },
    {
      "paper_id": "hf:2505.09343",
      "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
      "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
      "keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology"
      ],
      "url": "https://huggingface.co/papers/2505.09343",
      "published_at": "2025-05-14"
    },
    {
      "paper_id": "hf:2505.21600",
      "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
      "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ],
      "url": "https://huggingface.co/papers/2505.21600",
      "published_at": "2025-05-27"
    },
    {
      "paper_id": "hf:2505.09666",
      "title": "System Prompt Optimization with Meta-Learning",
      "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
      "keywords": [
        "Large Language Models (LLMs)",
        "bilevel system prompt optimization",
        "meta-learning",
        "system prompts",
        "user prompts",
        "unseen datasets",
        "unseen tasks",
        "rapid adaptation"
      ],
      "url": "https://huggingface.co/papers/2505.09666",
      "published_at": "2025-05-14"
    },
    {
      "paper_id": "hf:2505.23747",
      "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
      "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
      "keywords": [
        "spatial-mllm",
        "dual-encoder architecture",
        "visual geometry foundation model",
        "CLIP-based visual encoders",
        "semantic features",
        "3D structure features",
        "unified visual tokens",
        "space-aware frame sampling",
        "supervised fine-tuning",
        "GRPO",
        "spatial understanding",
        "spatial reasoning"
      ],
      "url": "https://huggingface.co/papers/2505.23747",
      "published_at": "2025-05-29"
    },
    {
      "paper_id": "hf:2505.19641",
      "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
      "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
      "keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "Logical Reasoning",
        "Data Synthesis",
        "BBEH",
        "Mixed Training",
        "DeepSeek-R1",
        "DeepSeek-R1-Distill-Qwen-32B",
        "DeepSeek-R1-Zero-Qwen-32B"
      ],
      "url": "https://huggingface.co/papers/2505.19641",
      "published_at": "2025-05-26"
    },
    {
      "paper_id": "hf:2505.04588",
      "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
      "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
      "keywords": [
        "reinforcement learning",
        "LLMs",
        "search capabilities",
        "document quality",
        "API costs",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "curriculum-based rollout strategy",
        "varying parameter sizes",
        "RL algorithms"
      ],
      "url": "https://huggingface.co/papers/2505.04588",
      "published_at": "2025-05-07"
    },
    {
      "paper_id": "hf:2505.19457",
      "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
      "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
      "keywords": [
        "large language models",
        "BizFinBench",
        "numerical calculation",
        "reasoning",
        "information extraction",
        "prediction recognition",
        "knowledge-based question answering",
        "IteraJudge",
        "Claude-3.5-Sonnet",
        "DeepSeek-R1",
        "Qwen2.5-VL-3B",
        "ChatGPT-o3",
        "Gemini-2.0-Flash",
        "Qwen3-1.7B"
      ],
      "url": "https://huggingface.co/papers/2505.19457",
      "published_at": "2025-05-26"
    },
    {
      "paper_id": "hf:2505.17225",
      "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
      "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
      "keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ],
      "url": "https://huggingface.co/papers/2505.17225",
      "published_at": "2025-05-22"
    },
    {
      "paper_id": "hf:2505.18445",
      "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data",
      "summary": "Diffusion models have advanced image stylization significantly, yet two core\nchallenges persist: (1) maintaining consistent stylization in complex scenes,\nparticularly identity, composition, and fine details, and (2) preventing style\ndegradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional\nstylization consistency highlights the performance gap between open-source\nmethods and proprietary models. To bridge this gap, we propose\nOmniConsistency, a universal consistency plugin leveraging large-scale\nDiffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context\nconsistency learning framework trained on aligned image pairs for robust\ngeneralization; (2) a two-stage progressive learning strategy decoupling style\nlearning from consistency preservation to mitigate style degradation; and (3) a\nfully plug-and-play design compatible with arbitrary style LoRAs under the Flux\nframework. Extensive experiments show that OmniConsistency significantly\nenhances visual coherence and aesthetic quality, achieving performance\ncomparable to commercial state-of-the-art model GPT-4o.",
      "keywords": [
        "diffusion models",
        "OmniConsistency",
        "Diffusion Transformers",
        "DiTs",
        "in-context consistency learning",
        "two-stage progressive learning",
        "style LoRAs",
        "Flux framework"
      ],
      "url": "https://huggingface.co/papers/2505.18445",
      "published_at": "2025-05-24"
    },
    {
      "paper_id": "hf:2505.14810",
      "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
      "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
      "keywords": [
        "instruction-following",
        "reasoning-oriented models",
        "benchmarks",
        "chains-of-thought",
        "reinforcement learning",
        "instruction adherence"
      ],
      "url": "https://huggingface.co/papers/2505.14810",
      "published_at": "2025-05-20"
    },
    {
      "paper_id": "hf:2505.21189",
      "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
      "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.",
      "keywords": [
        "large language models",
        "autoregressive generation",
        "input embedding",
        "frozen LLMs",
        "multi-token generation",
        "iterative decoding",
        "learned embeddings",
        "embedding space",
        "dedicated encoder"
      ],
      "url": "https://huggingface.co/papers/2505.21189",
      "published_at": "2025-05-27"
    },
    {
      "paper_id": "hf:2505.18129",
      "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
      "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
      "keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ],
      "url": "https://huggingface.co/papers/2505.18129",
      "published_at": "2025-05-23"
    },
    {
      "paper_id": "hf:2505.07747",
      "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
      "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
      "keywords": [
        "hybrid VAE-DiT",
        "diffusion-based texture synthesis",
        "TSDF representations",
        "perceiver-based latent encoding",
        "sharp edge sampling",
        "cross-view consistency",
        "geometric conditioning",
        "latent-space synchronization",
        "LoRA",
        "3D-native architecture"
      ],
      "url": "https://huggingface.co/papers/2505.07747",
      "published_at": "2025-05-12"
    },
    {
      "paper_id": "hf:2505.11049",
      "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
      "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
      "keywords": [
        "GuardReasoner-VL",
        "reasoning-based VLM guard model",
        "online RL",
        "SFT",
        "GuardReasoner-VLTrain",
        "rejection sampling",
        "safety-aware data concatenation",
        "dynamic clipping parameter",
        "length-aware safety reward",
        "F1 score"
      ],
      "url": "https://huggingface.co/papers/2505.11049",
      "published_at": "2025-05-16"
    }
  ],
  "clusters": [
    {
      "cluster_index": 0,
      "size": 4,
      "cohesion": 0.9211131036281586,
      "members": [
        {
          "paper_id": "hf:2505.17612",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9366630911827087
        },
        {
          "paper_id": "hf:2505.20411",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9365119934082031
        },
        {
          "paper_id": "hf:2505.15277",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9132559895515442
        },
        {
          "paper_id": "hf:2505.16938",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.8980213403701782
        }
      ]
    },
    {
      "cluster_index": 1,
      "size": 12,
      "cohesion": 0.9021764099597931,
      "members": [
        {
          "paper_id": "hf:2505.02567",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9370574355125427
        },
        {
          "paper_id": "hf:2505.09568",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9302496910095215
        },
        {
          "paper_id": "hf:2505.15809",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9242265224456787
        },
        {
          "paper_id": "hf:2505.14683",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.9140921831130981
        },
        {
          "paper_id": "hf:2505.19297",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.9019497632980347
        },
        {
          "paper_id": "hf:2505.07062",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.8978776931762695
        },
        {
          "paper_id": "hf:2505.05470",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.8973748683929443
        },
        {
          "paper_id": "hf:2505.23747",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.8960700631141663
        },
        {
          "paper_id": "hf:2505.21497",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.8926595449447632
        },
        {
          "paper_id": "hf:2505.07747",
          "rank_in_cluster": 9,
          "sim_to_centroid": 0.889878511428833
        },
        {
          "paper_id": "hf:2505.18445",
          "rank_in_cluster": 10,
          "sim_to_centroid": 0.8822569847106934
        },
        {
          "paper_id": "hf:2505.18125",
          "rank_in_cluster": 11,
          "sim_to_centroid": 0.8624236583709717
        }
      ]
    },
    {
      "cluster_index": 2,
      "size": 17,
      "cohesion": 0.9144929892876569,
      "members": [
        {
          "paper_id": "hf:2505.17667",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9358359575271606
        },
        {
          "paper_id": "hf:2505.19641",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9353216886520386
        },
        {
          "paper_id": "hf:2505.14810",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9311419725418091
        },
        {
          "paper_id": "hf:2505.10554",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.9288906455039978
        },
        {
          "paper_id": "hf:2505.23621",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.928024411201477
        },
        {
          "paper_id": "hf:2505.02387",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.9270680546760559
        },
        {
          "paper_id": "hf:2505.04921",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.9226415157318115
        },
        {
          "paper_id": "hf:2505.07608",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.9223319292068481
        },
        {
          "paper_id": "hf:2505.03335",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.9141044616699219
        },
        {
          "paper_id": "hf:2505.22617",
          "rank_in_cluster": 9,
          "sim_to_centroid": 0.9123984575271606
        },
        {
          "paper_id": "hf:2505.13417",
          "rank_in_cluster": 10,
          "sim_to_centroid": 0.9095466136932373
        },
        {
          "paper_id": "hf:2505.03318",
          "rank_in_cluster": 11,
          "sim_to_centroid": 0.906635046005249
        },
        {
          "paper_id": "hf:2505.21327",
          "rank_in_cluster": 12,
          "sim_to_centroid": 0.9031318426132202
        },
        {
          "paper_id": "hf:2504.20752",
          "rank_in_cluster": 13,
          "sim_to_centroid": 0.9010664224624634
        },
        {
          "paper_id": "hf:2505.17225",
          "rank_in_cluster": 14,
          "sim_to_centroid": 0.8908014893531799
        },
        {
          "paper_id": "hf:2505.18129",
          "rank_in_cluster": 15,
          "sim_to_centroid": 0.888990581035614
        },
        {
          "paper_id": "hf:2505.11049",
          "rank_in_cluster": 16,
          "sim_to_centroid": 0.8884497284889221
        }
      ]
    },
    {
      "cluster_index": 3,
      "size": 5,
      "cohesion": 0.9262909531593323,
      "members": [
        {
          "paper_id": "hf:2505.14669",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9353476762771606
        },
        {
          "paper_id": "hf:2505.09343",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9305588006973267
        },
        {
          "paper_id": "hf:2505.19147",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.926849365234375
        },
        {
          "paper_id": "hf:2505.11594",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.9208542108535767
        },
        {
          "paper_id": "hf:2505.10475",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.9178447127342224
        }
      ]
    },
    {
      "cluster_index": 4,
      "size": 12,
      "cohesion": 0.8926646560430527,
      "members": [
        {
          "paper_id": "hf:2505.04620",
          "rank_in_cluster": 0,
          "sim_to_centroid": 0.9234026670455933
        },
        {
          "paper_id": "hf:2505.21600",
          "rank_in_cluster": 1,
          "sim_to_centroid": 0.9109108448028564
        },
        {
          "paper_id": "hf:2505.21189",
          "rank_in_cluster": 2,
          "sim_to_centroid": 0.9085566997528076
        },
        {
          "paper_id": "hf:2505.11820",
          "rank_in_cluster": 3,
          "sim_to_centroid": 0.8980461359024048
        },
        {
          "paper_id": "hf:2505.09388",
          "rank_in_cluster": 4,
          "sim_to_centroid": 0.897467851638794
        },
        {
          "paper_id": "hf:2505.04588",
          "rank_in_cluster": 5,
          "sim_to_centroid": 0.8937560319900513
        },
        {
          "paper_id": "hf:2505.17894",
          "rank_in_cluster": 6,
          "sim_to_centroid": 0.8853275179862976
        },
        {
          "paper_id": "hf:2505.14302",
          "rank_in_cluster": 7,
          "sim_to_centroid": 0.8848555088043213
        },
        {
          "paper_id": "hf:2505.09666",
          "rank_in_cluster": 8,
          "sim_to_centroid": 0.8841605186462402
        },
        {
          "paper_id": "hf:2505.19457",
          "rank_in_cluster": 9,
          "sim_to_centroid": 0.879194974899292
        },
        {
          "paper_id": "hf:2505.02707",
          "rank_in_cluster": 10,
          "sim_to_centroid": 0.8785021901130676
        },
        {
          "paper_id": "hf:2505.07916",
          "rank_in_cluster": 11,
          "sim_to_centroid": 0.867794930934906
        }
      ]
    }
  ]
}